import os
import csv
import yaml
import shutil
from snakemake.io import expand
import pandas as pd
import glob
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from pathlib import Path


# ===== CONFIGURATION AND VALIDATION =====

# Config validation
def validate_config(config):
    required_keys = ['samples_file', 'output_dir', 'r', 's', 'run_name', 'mge_path']
    missing_keys = [key for key in required_keys if key not in config]
    
    if missing_keys:
        sys.stderr.write(f"WARNING: Missing required configuration keys: {', '.join(missing_keys)}\n")
        sys.exit(1)
    
    # Validate file existence
    if not os.path.exists(config['samples_file']):
        sys.stderr.write(f"WARNING: Samples file not found: {config['samples_file']}\n")
        sys.exit(1)
    
    if not os.path.exists(config['mge_path']):
        sys.stderr.write(f"WARNING: MitoGeneExtractor path not found: {config['mge_path']}\n")
        sys.exit(1)
    
    # Validate gene_fetch configuration
    run_gene_fetch = config.get("run_gene_fetch", False)
    if not isinstance(run_gene_fetch, bool):
        sys.stderr.write("WARNING: 'run_gene_fetch' must be a boolean (true/false)\n")
        sys.exit(1)
    
    if run_gene_fetch:
        # Validate gene_fetch parameters
        gene_fetch_config = config.get("gene_fetch", {})
        required_gene_fetch_keys = ['email', 'api_key', 'gene', 'input_type']
        missing_gene_fetch_keys = [key for key in required_gene_fetch_keys if key not in gene_fetch_config or not gene_fetch_config[key]]
        
        if missing_gene_fetch_keys:
            sys.stderr.write(f"WARNING: Missing required gene_fetch configuration parameters: {', '.join(missing_gene_fetch_keys)}\n")
            sys.exit(1)
            
        # Validate input_type value
        input_type = gene_fetch_config.get("input_type", "").lower()
        if input_type not in ["taxid", "hierarchical"]:
            sys.stderr.write("WARNING: 'input_type' must be either 'taxid' or 'hierarchical'\n")
            sys.exit(1)
            
        # Validate samples_file exists
        samples_file = gene_fetch_config.get("samples_file", config.get("samples_file", ""))
        if not samples_file or not os.path.exists(samples_file):
            sys.stderr.write(f"WARNING: Gene fetch samples file not found: {samples_file}\n")
            sys.exit(1)
    else:
        # If not using gene_fetch, validate sequence_reference_file exists
        if 'sequence_reference_file' not in config:
            sys.stderr.write("WARNING: 'sequence_reference_file' is required when run_gene_fetch is false\n")
            sys.exit(1)
        
        if not os.path.exists(config['sequence_reference_file']):
            sys.stderr.write(f"WARNING: Sequence reference file not found: {config['sequence_reference_file']}\n")
            sys.exit(1)
    
    # Validate r and s parameters
    if not isinstance(config['r'], list) or not config['r']:
        sys.stderr.write("WARNING: 'r' must be a non-empty list\n")
        sys.exit(1)
    
    if not isinstance(config['s'], list) or not config['s']:
        sys.stderr.write("WARNING: 's' must be a non-empty list\n")
        sys.exit(1)
		
    # Validate run_fasta_compare parameter
    run_fasta_compare = config.get("run_fasta_compare", True)  # Default to True
    if not isinstance(run_fasta_compare, bool):
        sys.stderr.write("WARNING: 'run_fasta_compare' must be a boolean (true/false)\n")
        sys.exit(1)

# Validate configuration file
validate_config(config)

# Print configuration for debugging
print("\n=====Configuration loaded:=====\n", config) 




# ===== DATA PARSING FUNCTIONS =====

# Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples

# Parse references from CSV
def parse_sequence_references(sequence_reference_file):
    sequence_references = {}
    with open(sequence_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            protein_reference_path = row['protein_reference_path']
            nucleotide_reference_path = row['nucleotide_reference_path']
            sequence_references[sample_id] = {
                "reference_name": reference_name, 
                "protein_path": protein_reference_path,
                "nucleotide_path": nucleotide_reference_path
            }    
    return sequence_references



# ===== PARAMETER SETUP & CONFIG PARSING =====

# Load parameters from config file
samples = parse_samples(config["samples_file"])
run_name = config["run_name"]

r = config["r"]
s = config["s"]
n = config["n"]
C = config["C"]
t = config["t"]
mge_path = config["mge_path"]


# Determine sequence reference file path based on gene_fetch setting
run_gene_fetch = config.get("run_gene_fetch", False)
if run_gene_fetch:
    sequence_reference_file_path = os.path.join(config["output_dir"], "references", "sequence_references.csv")
else:
    sequence_reference_file_path = config["sequence_reference_file"]



# Load fasta_cleaner parameters
fasta_cleaner_params = config.get("fasta_cleaner", {	
    "consensus_threshold": 0.5,
    "human_threshold": 0.95,
    "at_difference": 0.1,
    "at_mode": "absolute",
    "outlier_percentile": 90.0,
    "disable_human": False,
    "disable_at": False,
    "disable_outliers": False
})

# Check if fasta_compare should be performed
run_fasta_compare = config.get("run_fasta_compare", True)  # Default to True

# Load fasta_compare parameters
fasta_compare_params = config.get("fasta_compare", {
    "target": "cox1",
    "verbose": False
})

# Load rule resources 
rule_resources = config["rules"]

# Create main output directory
main_output_dir = config["output_dir"]
Path(main_output_dir).mkdir(parents=True, exist_ok=True)

# Create separate output directories for each preprocessing mode
output_dir_merge = os.path.join(main_output_dir, "merge_mode")
output_dir_concat = os.path.join(main_output_dir, "concat_mode")

# Create references directory if using gene_fetch
if run_gene_fetch:
    references_dir = os.path.join(main_output_dir, "references")
    Path(references_dir).mkdir(parents=True, exist_ok=True)

# Check if reference filtering should be performed
reference_dir = fasta_cleaner_params.get("reference_dir", None)
use_reference_filtering = reference_dir and reference_dir not in ["None", "null", "", None]



# ===== HELPER FUNCTIONS =====

# Handles sequence_reference_file dependency for MGE and file path
def get_protein_reference(wildcards):
    """Get protein reference path for a sample"""
    if run_gene_fetch:
        # Don't read the CSV - construct the path directly
        # Assumes gene_fetch creates files like: references/{sample}_protein.faa
        return os.path.join(main_output_dir, f"references/protein/{wildcards.sample}.fasta")
    else:
        # For manual references, we can read the file immediately since it exists
        sequence_refs = parse_sequence_references(config["sequence_reference_file"])
        return sequence_refs[wildcards.sample]["protein_path"]
	
# Generate all combinations for each sample+reference pair with all r and s values
def get_all_mge_consensus_files(mode):
    """Generate list of expected consensus files for a given mode"""
    output_dir = output_dir_merge if mode == "merge" else output_dir_concat
    files = []
    
    for sample in samples.keys():
        for r_val in r:
            for s_val in s:
                files.append(os.path.join(output_dir, f"consensus/{sample}_r_{r_val}_s_{s_val}_con_{sample}.fas"))
    
    return files

# Get the right input for merge mode
def get_mge_input_merge(wildcards):
    return os.path.join(output_dir_merge, f"trimmed_data/{wildcards.sample}_merged_clean.fq")

# Get input for concat mode
def get_mge_input_concat(wildcards):
    return os.path.join(output_dir_concat, f"trimmed_data/{wildcards.sample}/{wildcards.sample}_concat_trimmed.fq")



# ===== SNAKEMAKE RULES =====

# Rule them all
rule all:
    input:
        # ==== GENE FETCH OUTPUT (CONDITIONAL) ====
        (os.path.join(main_output_dir, "references/sequence_references.csv") if run_gene_fetch else []),
        
        # ==== MERGE MODE OUTPUTS ====
        os.path.join(output_dir_merge, "logs/clean_headers.log"),
        os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        os.path.join(output_dir_merge, "logs/mge/alignment_files.log"),
        os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv"),
        os.path.join(output_dir_merge, "logs/cleaning_complete.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        os.path.join(output_dir_merge, "logs/file_cleanup_complete.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
        os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
        os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        (os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filtered.txt") if use_reference_filtering else []),
        (os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv") if use_reference_filtering else []), 
        os.path.join(output_dir_merge, "fasta_cleaner/all_consensus_sequences.fasta"),
        os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv"),
        os.path.join(output_dir_merge, "logs/exonerate_cleanup_complete.txt"),
        
        # ==== CONCAT MODE OUTPUTS ====
        os.path.join(output_dir_concat, "logs/concat_reads.log"),
        os.path.join(output_dir_concat, "logs/trim_galore.log"),
        expand(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq.gz"), sample=list(samples.keys())),
        expand(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq.gz"), sample=list(samples.keys())), 
        os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        os.path.join(output_dir_concat, "logs/mge/alignment_files.log"),
        os.path.join(output_dir_concat, f"{run_name}_concat-stats.csv"),
        os.path.join(output_dir_concat, "logs/cleaning_complete.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
        os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
        os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        (os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filtered.txt") if use_reference_filtering else []),
        (os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv") if use_reference_filtering else []),
        os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta"),
        os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv"),
        os.path.join(output_dir_concat, "logs/exonerate_cleanup_complete.txt"),
		
        # ==== FASTA COMPARE OUTPUTS (CONDITIONAL) ====
        (os.path.join(main_output_dir, f"fasta_compare/{run_name}_fasta_compare.csv") if run_fasta_compare else []),
        (os.path.join(main_output_dir, f"fasta_compare/{run_name}_full_sequences.fasta") if run_fasta_compare else []),
        (os.path.join(main_output_dir, f"fasta_compare/{run_name}_barcode_sequences.fasta") if run_fasta_compare else []),
		
        # ==== FINAL OUTPUTS ====
        os.path.join(main_output_dir, f"{run_name}_combined_stats.csv"),
        os.path.join(output_dir_merge, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        os.path.join(output_dir_concat, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        os.path.join(output_dir_concat, "logs/file_cleanup_complete.txt"),


# ===== GENE FETCH RULE =====
if run_gene_fetch:
    rule gene_fetch:
        input:
            samples_csv=lambda wildcards: config["gene_fetch"].get("samples_file", config["samples_file"])
        output:
            sequence_references=os.path.join(main_output_dir, "references/sequence_references.csv"),
            protein_files=expand(
                os.path.join(main_output_dir, "references/protein/{sample}.fasta"),
                sample=list(samples.keys())
            )
        params:
            email=config["gene_fetch"]["email"],
            api_key=config["gene_fetch"]["api_key"],
            gene=config["gene_fetch"]["gene"],
            input_type=config["gene_fetch"]["input_type"].lower(),
            genbank=config["gene_fetch"].get("genbank", False),
            output_dir=os.path.join(main_output_dir, "references"),
            genbank_flag="--genbank" if config["gene_fetch"].get("genbank", False) else "",
            input_flag="--in" if config["gene_fetch"]["input_type"].lower() == "taxid" else "--in2"
        log:
            os.path.join(main_output_dir, "references/gene_fetch.log")
        threads: rule_resources["gene_fetch"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["gene_fetch"]["mem_mb"] * attempt
        retries: 3
        shell:
            """
            set -euo pipefail
            
            echo "Starting gene-fetch: $(date)" > {log}
            echo "Email: {params.email}" >> {log}
            echo "Gene: {params.gene}" >> {log}
            echo "Type: protein" >> {log}
            echo "Input type: {params.input_type}" >> {log}
            echo "Genbank: {params.genbank}" >> {log}
            echo "Input samples: {input.samples_csv}" >> {log}
            echo "Output directory: {params.output_dir}" >> {log}
            
            # Run gene-fetch
            gene-fetch \\
                --email {params.email} \\
                --api-key {params.api_key} \\
                {params.input_flag} {input.samples_csv} \\
                --gene {params.gene} \\
                --type protein \\
                {params.genbank_flag} \\
                --out {params.output_dir} \\
                >> {log} 2>&1
            
            echo "Gene-fetch completed: $(date)" >> {log}
            echo "Output file: {output.sequence_references}" >> {log}
            
            # Verify output file was created
            if [ ! -f "{output.sequence_references}" ]; then
                echo "ERROR: Expected output file not found: {output.sequence_references}" >> {log}
                exit 1
            fi
            """


# ===== MERGE MODE RULES =====
rule fastp_pe_merge:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_R1_trimmed.fq.gz")),
        R2_trimmed=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_R2_trimmed.fq.gz")),
        report=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_fastp_report.html")),
        json=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_fastp_report.json")),
        merged_reads=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_merged.fq")),
        unpaired_R1=temp(os.path.join(output_dir_merge, "trimmed_data/unpaired/{sample}_unpaired_R1.fq")),
        unpaired_R2=temp(os.path.join(output_dir_merge, "trimmed_data/unpaired/{sample}_unpaired_R2.fq"))
    log:
        out=os.path.join(output_dir_merge, "logs/fastp/{sample}_fastp.out"),
        err=os.path.join(output_dir_merge, "logs/fastp/{sample}_fastp.err")
    threads: rule_resources["fastp_pe_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fastp_pe_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
		
        # Log job info
        echo "Starting fastp merge for sample {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Input files: {input.R1}, {input.R2}" >> {log.out}

        fastp -i {input.R1} -I {input.R2} \
              -o {output.R1_trimmed} -O {output.R2_trimmed} \
              --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
              --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
              --dedup \
              --trim_poly_g \
              --thread {threads} \
              --merge --merged_out {output.merged_reads} \
              --unpaired1 {output.unpaired_R1} \
              --unpaired2 {output.unpaired_R2} \
              -h {output.report} -j {output.json} \
              > {log.out} 2> {log.err}

        # Log job completion and disk usage
        echo "Completed: $(date)" >> {log.out}
        echo "Output files sizes:" >> {log.out}
        du -h {output.R1_trimmed} {output.R2_trimmed} {output.merged_reads} >> {log.out}
        """

rule clean_headers_merge:
    input:
        merged_reads=os.path.join(output_dir_merge, "trimmed_data/{sample}_merged.fq")
    output:
        clean_merged=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_merged_clean.fq"))
    log:
        out=temp(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.out")),
        err=temp(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.err"))
    threads: rule_resources["clean_headers_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["clean_headers_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        # Create directories first to avoid race conditions
        mkdir -p $(dirname {log.out})
        mkdir -p $(dirname {output.clean_merged})

        # Create log header with timestamp and sample ID
        echo "Cleaning headers for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Host: $(hostname)" >> {log.out}

        # Add robust error handling with retries
        for attempt in $(seq 1 3); do
            echo "Attempt $attempt of 3" >> {log.out}

            if ! [ -r "{input.merged_reads}" ]; then
                echo "Input file not readable, waiting 10 seconds..." >> {log.out}
                sleep 10
                continue
            fi

            # Use perl with temporary file approach to be more robust
            TMP_OUTPUT=$(mktemp)
            if perl -pe 's/ /_/g' {input.merged_reads} > $TMP_OUTPUT 2>> {log.err}; then # Redirect perl errors
                # Move the temp file to the final location only if successful
                mv $TMP_OUTPUT {output.clean_merged}
                echo "Completed: $(date)" >> {log.out}
                echo "Output file size: $(du -h {output.clean_merged} | cut -f1)" >> {log.out}
                echo "--------------------------------------------" >> {log.out}
                exit 0
            else
                echo "Perl command failed on attempt $attempt" >> {log.out}
                rm -f $TMP_OUTPUT
                sleep 5
            fi
        done

        # If all attempts fail
        echo "FAILED after 3 attempts: $(date)" >> {log.out}
        exit 1
        """

rule aggregate_clean_headers_logs_merge:
    input:
        sample_logs=expand(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.out"), sample=list(samples.keys()))
    output:
        combined_log=os.path.join(output_dir_merge, "logs/clean_headers.log") 
    log:
        out=os.path.join(output_dir_merge, "logs/aggregate_clean_headers.out"),
        err=os.path.join(output_dir_merge, "logs/aggregate_clean_headers.err")
    threads: rule_resources["aggregate_clean_headers_logs_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["aggregate_clean_headers_logs_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        # Create header for aggregated log file
        echo "===============================================" > {output.combined_log}
        echo "Aggregated clean headers logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}

        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log} 2>> {log.err}
        echo "Aggregation complete." >> {log.out}
        """

# Helper function to get the right input for merge mode
def get_mge_input_merge(wildcards):
    return os.path.join(output_dir_merge, f"trimmed_data/{wildcards.sample}_merged_clean.fq")


# ===== CONCAT MODE RULES =====

rule fastp_pe_concat:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq"),
        R2_trimmed=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq"),
        report=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_fastp_report.html"),
        json=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_fastp_report.json")
    log:
        out=os.path.join(output_dir_concat, "logs/fastp/{sample}_fastp.out"),
        err=os.path.join(output_dir_concat, "logs/fastp/{sample}_fastp.err")
    threads: rule_resources["fastp_pe_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fastp_pe_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        # Create output directories if they don't exist
        mkdir -p $(dirname {output.R1_trimmed})
        mkdir -p $(dirname {output.report})
        
        # Create a consistent log header with timestamp and sample ID
        LOG_HEADER="[$(date '+%Y-%m-%d %H:%M:%S')] Sample: {wildcards.sample}"
        
        # Initialize the log files
        echo "$LOG_HEADER - Starting fastp processing (attempt ${{SLURM_RESTART_COUNT:-1}})" > {log.out}
        echo "$LOG_HEADER - Input files: {input.R1}, {input.R2}" >> {log.out}
        echo "$LOG_HEADER - Output files: {output.R1_trimmed}, {output.R2_trimmed}" >> {log.out}
        
        # Log input file sizes and existence
        echo "$LOG_HEADER - Input file details:" >> {log.out}
        if [[ -f "{input.R1}" ]]; then
            echo "  R1: $(du -h {input.R1} | cut -f1) ($(stat -c%s {input.R1}) bytes)" >> {log.out}
        else
            echo "  ERROR: R1 input file does not exist" >> {log.out}
            exit 1
        fi
        
        if [[ -f "{input.R2}" ]]; then
            echo "  R2: $(du -h {input.R2} | cut -f1) ($(stat -c%s {input.R2}) bytes)" >> {log.out}
        else
            echo "  ERROR: R2 input file does not exist" >> {log.out}
            exit 1
        fi
        
        # Run fastp with comprehensive logging
        echo "$LOG_HEADER - Starting fastp: $(date)" >> {log.out}
        
        # Execute fastp with error handling
        if fastp -i {input.R1} -I {input.R2} \\
                 -o {output.R1_trimmed} -O {output.R2_trimmed} \\
                 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \\
                 -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \\
                 --dedup \\
                 --trim_poly_g \\
                 --thread {threads} \\
                 -h {output.report} -j {output.json} \\
                 >> {log.out} 2> {log.err}; then
            echo "$LOG_HEADER - Fastp completed successfully: $(date)" >> {log.out}
        else
            FASTP_EXIT=$?
            echo "$LOG_HEADER - ERROR: Fastp failed with exit code $FASTP_EXIT: $(date)" >> {log.out}
            
            # Examine error log for specific issues
            if grep -q "different read numbers" {log.err}; then
                echo "$LOG_HEADER - ERROR: Read count mismatch detected in input files" >> {log.out}
                echo "$LOG_HEADER - Error details from fastp:" >> {log.out}
                grep -A 5 "different read numbers" {log.err} >> {log.out}
            fi
            
            # Check if output files were at least partially created
            if [[ -f {output.R1_trimmed} ]]; then
                echo "$LOG_HEADER - R1 output was created but may be incomplete: $(du -h {output.R1_trimmed} | cut -f1)" >> {log.out}
            fi
            
            if [[ -f {output.R2_trimmed} ]]; then
                echo "$LOG_HEADER - R2 output was created but may be incomplete: $(du -h {output.R2_trimmed} | cut -f1)" >> {log.out}
            fi
            
            # Propagate the error to trigger a retry
            exit $FASTP_EXIT
        fi
        
        # Validate output files
        if [[ ! -s {output.R1_trimmed} ]]; then
            echo "$LOG_HEADER - ERROR: R1 output file is empty after processing" >> {log.out}
            exit 1
        fi
        
        if [[ ! -s {output.R2_trimmed} ]]; then
            echo "$LOG_HEADER - ERROR: R2 output file is empty after processing" >> {log.out}
            exit 1
        fi
        
        # Calculate and log read counts from output files
        echo "$LOG_HEADER - Calculating read counts in output files..." >> {log.out}
        R1_READS=$(($(wc -l < {output.R1_trimmed}) / 4))
        R2_READS=$(($(wc -l < {output.R2_trimmed}) / 4))
        echo "$LOG_HEADER - Read counts: R1=$R1_READS reads, R2=$R2_READS reads" >> {log.out}
        
        # Verify read counts match between R1 and R2
        if [[ $R1_READS -ne $R2_READS ]]; then
            echo "$LOG_HEADER - WARNING: Read count mismatch in output files (R1=$R1_READS, R2=$R2_READS)" >> {log.out}
            # Not failing here since the files were created, but logging the issue
        fi
        
        # Clean headers
        echo "$LOG_HEADER - Cleaning headers in output files" >> {log.out}
        perl -i -pe 's/ /_/g' {output.R1_trimmed}
        perl -i -pe 's/ /_/g' {output.R2_trimmed}
        echo "$LOG_HEADER - Headers cleaned" >> {log.out}
        
        # Log detailed output file information
        echo "$LOG_HEADER - Output file details:" >> {log.out}
        echo "  R1: $(du -h {output.R1_trimmed} | cut -f1) ($(stat -c%s {output.R1_trimmed}) bytes)" >> {log.out}
        echo "  R2: $(du -h {output.R2_trimmed} | cut -f1) ($(stat -c%s {output.R2_trimmed}) bytes)" >> {log.out}
        
        # Log successful completion
        echo "$LOG_HEADER - Processing completed successfully at $(date)" >> {log.out}
        echo "$LOG_HEADER - ----------------------------------------" >> {log.out}
        """

rule fastq_concat:
    input:
        R1=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq"),
        R2=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq")
    output:
        temp(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq"))
    log:
        out=os.path.join(output_dir_concat, "logs/concat/{sample}.out")
    threads: rule_resources["fastq_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fastq_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
                
        echo "Concatenating files for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}

        # Concatenation
        (cat {input.R1} && cat {input.R2}) > {output} 2>> {log.out}

        echo "Completed: $(date)" >> {log.out}
        echo "Output file size: $(du -h {output} | cut -f1)" >> {log.out}
        """


rule gzip_trimmed:
    input:
        R1=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq"),
        R2=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq"),
        concat=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq")
    output:
        R1_gz=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq.gz"),
        R2_gz=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq.gz")
    log:
        out=os.path.join(output_dir_concat, "logs/gzip/{sample}.out")
    threads: rule_resources["gzip_trimmed"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["gzip_trimmed"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
        
        echo "Compressing trimmed files for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}

        # Compress the trimmed files using pigz for parallel compression
        echo "Compressing R1 file with pigz (using {threads} threads)..." >> {log.out}
        pigz -c -p {threads} {input.R1} > {output.R1_gz} 2>> {log.out}
        
        echo "Compressing R2 file with pigz (using {threads} threads)..." >> {log.out}
        pigz -c -p {threads} {input.R2} > {output.R2_gz} 2>> {log.out}

        # Remove the original uncompressed files
        echo "Removing original uncompressed files..." >> {log.out}
        rm {input.R1} {input.R2}

        echo "Compression completed: $(date)" >> {log.out}
        echo "Compressed file sizes:" >> {log.out}
        echo "  R1: $(du -h {output.R1_gz} | cut -f1)" >> {log.out}
        echo "  R2: $(du -h {output.R2_gz} | cut -f1)" >> {log.out}
        """

rule aggregate_concat_logs:
    input:
        sample_logs=expand(os.path.join(output_dir_concat, "logs/concat/{sample}.out"), sample=list(samples.keys()))
    output:
        combined_log=os.path.join(output_dir_concat, "logs/concat_reads.log")
    log:
        out=os.path.join(output_dir_concat, "logs/aggregate_concat.out"),
        err=os.path.join(output_dir_concat, "logs/aggregate_concat.err")
    threads: rule_resources["aggregate_concat_logs"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["aggregate_concat_logs"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        echo "===============================================" > {output.combined_log}
        echo "Aggregated concat logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}

        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log} 2>> {log.err}
        echo "Aggregation complete." >> {log.out}
        """


rule quality_trim:
    input:
        os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq")
    output:
        temp(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat_trimmed.fq")),
        report=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq_trimming_report.txt")
    params:
        outdir=os.path.join(output_dir_concat, "trimmed_data/{sample}"),
        report_dir=os.path.join(output_dir_concat, "trimmed_data/")
    log:
        out=os.path.join(output_dir_concat, "logs/trim_galore/{sample}.out"),
        err=os.path.join(output_dir_concat, "logs/trim_galore/{sample}.err")
    threads: rule_resources["quality_trim"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["quality_trim"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        echo "Quality trimming for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Input file: {input}" >> {log.out}

        # Run trim_galore
        trim_galore --cores {threads} \\
                    --dont_gzip \\
                    --output_dir {params.outdir} \\
                    --basename {wildcards.sample}_concat \\
                    {input} >> {log.out} 2>> {log.err}

        # Move reports to out location
        #mv {params.outdir}/{wildcards.sample}_concat.fastq_trimming_report.txt {output.report} >> {log.out} 2>> {log.err}

        # Log job completion
        echo "Completed: $(date)" >> {log.out}
        echo "Output file size: $(du -h {output[0]} | cut -f1)" >> {log.out}
        """

rule aggregate_trim_galore_logs:
    input:
        sample_logs=expand(os.path.join(output_dir_concat, "logs/trim_galore/{sample}.out"), sample=list(samples.keys())) # Use .out
    output:
        combined_log=os.path.join(output_dir_concat, "logs/trim_galore.log")
    log:
        out=os.path.join(output_dir_concat, "logs/aggregate_trim_galore.out"),
        err=os.path.join(output_dir_concat, "logs/aggregate_trim_galore.err")
    threads: rule_resources["aggregate_trim_galore_logs"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["aggregate_trim_galore_logs"]["mem_mb"] * attempt
    shell:
        """
        set -euo pipefail

        echo "===============================================" > {output.combined_log}
        echo "Aggregated trim_galore logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}

        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log} 2>> {log.err}
        echo "Aggregation complete." >> {log.out}
        """

# Helper function to get the right input for concat mode
def get_mge_input_concat(wildcards):
    return os.path.join(output_dir_concat, f"trimmed_data/{wildcards.sample}/{wildcards.sample}_concat_trimmed.fq")





# ===== SHARED RULES WITH MODE-SPECIFIC VERSIONS =====

# MGE rule for merge mode
rule MitoGeneExtractor_merge:
    input:
        DNA=get_mge_input_merge,
        AA=get_protein_reference
    output:
        alignment=os.path.join(output_dir_merge, "alignment/{sample}_r_{r}_s_{s}_align_{sample}.fas"),
        consensus=os.path.join(output_dir_merge, "consensus/{sample}_r_{r}_s_{s}_con_{sample}.fas")
    log:
        out=os.path.join(output_dir_merge, "out/{sample}_r_{r}_s_{s}_summary.out"),
        err=os.path.join(output_dir_merge, "err/{sample}_r_{r}_s_{s}_summary.err")
    params:
        mge_executor=config["mge_path"],
        n=config["n"],
        C=config["C"],
        t=config["t"],
        output_dir=output_dir_merge,
        vulgar_dir=lambda wildcards: os.path.join(output_dir_merge, f"logs/mge/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}/")
    threads: rule_resources["MitoGeneExtractor_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["MitoGeneExtractor_merge"]["mem_mb"] * attempt,
        slurm_partition=rule_resources["MitoGeneExtractor_merge"]["partition"]
    retries: 4
    shell:
        """
        set -euo pipefail

        # Check if input file is empty or missing
        if [ ! -s "{input.DNA}" ]; then
            echo "===== EMPTY/MISSING INPUT DETECTED =====" >> {log.out}
            echo "Started: $(date)" >> {log.out}
            echo "Sample: {wildcards.sample}" >> {log.out}
            echo "Mode: merge" >> {log.out}
            echo "Input file: {input.DNA}" >> {log.out}
            
            if [ ! -f "{input.DNA}" ]; then
                echo "Input file does not exist" >> {log.out}
            else
                FILE_SIZE=$(stat -c%s "{input.DNA}" 2>/dev/null || echo "unknown")
                echo "Input file size: $FILE_SIZE bytes" >> {log.out}
                echo "File is empty - likely all reads were filtered out during trimming" >> {log.out}
            fi
            
            echo "Creating mock outputs..." >> {log.out}
            
            
            # Mock outputs (consensus = header only)
            echo ">Consensus__{wildcards.sample}" > {output.consensus}
            touch {output.alignment}
            touch {log.err}
            echo "Mock outputs created successfully: $(date)" >> {log.out}
            echo "=========================================" >> {log.out}
            exit 0
        fi

        # Create vulgar directory if it doesn't exist
        mkdir -p {params.vulgar_dir}
        
        # Normal execution path - input has content
        echo "===== Job Info =====" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Node: $(hostname)" >> {log.out}
        echo "Memory allocated: {resources.mem_mb}MB" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "Mode: merge" >> {log.out}
        echo "===================" >> {log.out}
        
        # Log input file details
        echo "Input DNA file size: $(du -h {input.DNA} | cut -f1)" >> {log.out}
        echo "Number of sequences: $(grep -c "^@" {input.DNA} || echo "0")" >> {log.out}
        echo "Input AA file: {input.AA}" >> {log.out}
        echo "Available system memory before execution:" >> {log.out}
        free -h >> {log.out}
             
        # Run MGE & track resource usage
        time {params.mge_executor} \\
        -q {input.DNA} -p {input.AA} \\
        -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \\
        -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \\
        -V {params.vulgar_dir} \\
        -r {wildcards.r} -s {wildcards.s} \\
        -n {params.n} \\
        -C {params.C} \\
        -t {params.t} \\
        --temporaryDirectory {params.output_dir} \\
        --verbosity 10 \\
        >> {log.out} 2>> {log.err} || {{
            EXIT_CODE=$?
            echo "MitoGeneExtractor failed with exit code $EXIT_CODE" >> {log.out}
            echo "Available system memory after failure:" >> {log.out}
            free -h >> {log.out}
            echo "System load:" >> {log.out}
            uptime >> {log.out}
            echo "Top memory processes:" >> {log.out}
            ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -n 10 >> {log.out}
            exit $EXIT_CODE
        }}

        echo "Job completed successfully: $(date)" >> {log.out}
        echo "Available system memory after execution:" >> {log.out}
        free -h >> {log.out}
        
        echo "Output file sizes:" >> {log.out}
        if [ -f "{output.consensus}" ] && [ -f "{output.alignment}" ]; then
            du -h {output.consensus} {output.alignment} >> {log.out}
        else
            echo "Warning: Output files not found. Check for errors." >> {log.out}
        fi
        """

# MGE rule for concat mode
rule MitoGeneExtractor_concat:
    input:
        DNA=get_mge_input_concat,
        AA=get_protein_reference
    output:
        alignment=os.path.join(output_dir_concat, "alignment/{sample}_r_{r}_s_{s}_align_{sample}.fas"),
        consensus=os.path.join(output_dir_concat, "consensus/{sample}_r_{r}_s_{s}_con_{sample}.fas")
    log:
        out=os.path.join(output_dir_concat, "out/{sample}_r_{r}_s_{s}_summary.out"),
        err=os.path.join(output_dir_concat, "err/{sample}_r_{r}_s_{s}_summary.err")
    params:
        mge_executor=config["mge_path"],
        n=config["n"],
        C=config["C"],
        t=config["t"],		
        output_dir=output_dir_concat,
        vulgar_dir=lambda wildcards: os.path.join(output_dir_concat, f"logs/mge/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}/")
    threads: rule_resources["MitoGeneExtractor_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["MitoGeneExtractor_concat"]["mem_mb"] * attempt,
        slurm_partition=rule_resources["MitoGeneExtractor_concat"]["partition"]
    retries: 4
    shell:
        """
        set -euo pipefail

        # Check if input file is empty or missing
        if [ ! -s "{input.DNA}" ]; then
            echo "===== EMPTY/MISSING INPUT DETECTED =====" >> {log.out}
            echo "Started: $(date)" >> {log.out}
            echo "Sample: {wildcards.sample}" >> {log.out}
            echo "Mode: concat" >> {log.out}
            echo "Input file: {input.DNA}" >> {log.out}
            
            if [ ! -f "{input.DNA}" ]; then
                echo "Input file does not exist" >> {log.out}
            else
                FILE_SIZE=$(stat -c%s "{input.DNA}" 2>/dev/null || echo "unknown")
                echo "Input file size: $FILE_SIZE bytes" >> {log.out}
                echo "File is empty - likely all reads were filtered out during trimming" >> {log.out}
            fi
            
            echo "Creating mock outputs..." >> {log.out}

            # Mock outputs (consensus = header only)
            echo ">Consensus__{wildcards.sample}" > {output.consensus}
            touch {output.alignment}
            touch {log.err}
            echo "Mock outputs created successfully: $(date)" >> {log.out}
            echo "=========================================" >> {log.out}
            exit 0
        fi

        # Create vulgar directory if it doesn't exist
        mkdir -p {params.vulgar_dir}
        
        # Normal execution path - input has content
        echo "===== Job Info =====" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Node: $(hostname)" >> {log.out}
        echo "Memory allocated: {resources.mem_mb}MB" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "Mode: concat" >> {log.out}
        echo "===================" >> {log.out}
        
        # Log input file details
        echo "Input DNA file size: $(du -h {input.DNA} | cut -f1)" >> {log.out}
        echo "Number of sequences: $(grep -c "^@" {input.DNA} || echo "0")" >> {log.out}
        echo "Input AA file: {input.AA}" >> {log.out}
        echo "Available system memory before execution:" >> {log.out}
        free -h >> {log.out}
            
        # Run MGE & track resource usage
        time {params.mge_executor} \\
        -q {input.DNA} -p {input.AA} \\
        -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \\
        -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \\
        -V {params.vulgar_dir} \\
        -r {wildcards.r} -s {wildcards.s} \\
        -n {params.n} \\
        -C {params.C} \\
        -t {params.t} \\
        --temporaryDirectory {params.output_dir} \\
        --verbosity 10 \\
        >> {log.out} 2>> {log.err} || {{
            EXIT_CODE=$?
            echo "MitoGeneExtractor failed with exit code $EXIT_CODE" >> {log.out}
            echo "Available system memory after failure:" >> {log.out}
            free -h >> {log.out}
            echo "System load:" >> {log.out}
            uptime >> {log.out}
            exit $EXIT_CODE
        }}
        
        echo "Job completed successfully: $(date)" >> {log.out}
        echo "Available system memory after execution:" >> {log.out}
        free -h >> {log.out}
        
        echo "Output file sizes:" >> {log.out}
        if [ -f "{output.consensus}" ] && [ -f "{output.alignment}" ]; then
            du -h {output.consensus} {output.alignment} >> {log.out}
        else
            echo "Warning: Output files not found. Check for errors." >> {log.out}
        fi
        """





# Rename headers in consensus files for merge mode
rule rename_and_combine_cons_merge:
    input:
        consensus_files=get_all_mge_consensus_files("merge")
    output:
        complete=os.path.join(output_dir_merge, "logs/rename_complete.txt"),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta")
    log:
        os.path.join(output_dir_merge, "logs/rename_fasta.log")
    params:
        script=os.path.join(workflow.basedir, "scripts/rename_headers.py"),
        preprocessing_mode="merge"
    threads: rule_resources["rename_and_combine_cons_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["rename_and_combine_cons_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """ 
        set -euo pipefail

		# Check all input files exist
        for file in {input.consensus_files}; do
            if [ ! -f "$file" ]; then
                echo "Error: Required input file does not exist: $file" >> {log}
                echo "Cannot proceed with renaming and combining consensus files." >> {log}
				echo "If any MitoGeneExtractor jobs failed, please rerun the workflow to generate those files." >> {log}

                exit 1
            fi
        done
		
        # Run script
        python {params.script} \\
            --input-files {input.consensus_files} \\
            --concatenated-consensus {output.concat_cons} \\
            --complete-file={output.complete} \\
            --log-file={log} \\
            --preprocessing-mode={params.preprocessing_mode} \\
            --threads={threads}
    
        # Touch completion file to ensure timestamp is updated
        touch {output.complete}
        """

# Rename headers in consensus files for concat mode
rule rename_and_combine_cons_concat:
    input:
        consensus_files=get_all_mge_consensus_files("merge")
    output:
        complete=os.path.join(output_dir_concat, "logs/rename_complete.txt"),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta")
    log:
        os.path.join(output_dir_concat, "logs/rename_fasta.log")
    params:
        script=os.path.join(workflow.basedir, "scripts/rename_headers.py"),
        preprocessing_mode="concat"
    threads: rule_resources["rename_and_combine_cons_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["rename_and_combine_cons_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """ 
        set -euo pipefail

		# Check all input files exist
        for file in {input.consensus_files}; do
            if [ ! -f "$file" ]; then
                echo "Error: Required input file does not exist: $file" >> {log}
                echo "Cannot proceed with renaming and combining consensus files." >> {log}
				echo "If any MitoGeneExtractor jobs failed, please rerun the workflow to generate those files." >> {log}
                exit 1
            fi
        done
		
        # Run script
        python {params.script} \\
            --input-files {input.consensus_files} \\
            --concatenated-consensus {output.concat_cons} \\
            --complete-file={output.complete} \\
            --log-file={log} \\
            --preprocessing-mode={params.preprocessing_mode} \\
            --threads={threads}
    
        # Touch completion file to ensure timestamp is updated
        touch {output.complete}
        """



# Remove exonerate intermediate files after consensus generation
rule remove_exonerate_intermediates:
    input:
        merge_consensus=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        concat_consensus=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        merge_rename_complete=os.path.join(output_dir_merge, "logs/rename_complete.txt"),
        concat_rename_complete=os.path.join(output_dir_concat, "logs/rename_complete.txt")
    output:
        merge_completion=os.path.join(output_dir_merge, "logs/exonerate_cleanup_complete.txt"),
        concat_completion=os.path.join(output_dir_concat, "logs/exonerate_cleanup_complete.txt")
    threads: rule_resources["remove_exonerate_intermediates"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["remove_exonerate_intermediates"]["mem_mb"] * attempt
    retries: 2
    run:     
        # Clean each mode directory and write separate completion files
        for mode_dir, mode_name, completion_file in [
            (output_dir_merge, "merge", output.merge_completion),
            (output_dir_concat, "concat", output.concat_completion)
        ]:
            removed_files = 0
            total_size_saved = 0
            
            with open(completion_file, 'w') as log:
                log.write(f"Starting exonerate intermediate file cleanup for {mode_name} mode: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                log.write(f"Mode directory: {mode_dir}\n\n")
                
                exonerate_pattern = os.path.join(mode_dir, "Concatenated_exonerate_input_*")
                exonerate_files = glob.glob(exonerate_pattern)
                
                if not exonerate_files:
                    log.write(f"No Concatenated_exonerate_input_* files found\n")
                else:
                    log.write(f"Found {len(exonerate_files)} files to remove:\n")
                    
                    for file_path in exonerate_files:
                        try:
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            
                            size_mb = file_size / (1024 * 1024)
                            log.write(f"  - Removed: {os.path.basename(file_path)} ({size_mb:.2f} MB)\n")
                            
                            removed_files += 1
                            total_size_saved += file_size
                            
                        except OSError as e:
                            log.write(f"  - ERROR removing {file_path}: {e}\n")
                
                # Summary
                total_saved_mb = total_size_saved / (1024 * 1024)
                log.write(f"\nCLEANUP SUMMARY for {mode_name} mode:\n")
                log.write(f"  Total files removed: {removed_files}\n")
                log.write(f"  Total disk space saved: {total_saved_mb:.2f} MB\n")
                log.write(f"  Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
			
			

# Create list of alignment files for stats for merge mode
rule create_alignment_log_merge:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir_merge, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta")
    output:
        alignment_log=os.path.join(output_dir_merge, "logs/mge/alignment_files.log")
    threads: rule_resources["create_alignment_log_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["create_alignment_log_merge"]["mem_mb"] * attempt
    retries: 3
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")

# Create list of alignment files for stats for concat mode
rule create_alignment_log_concat:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir_concat, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta")
    output:
        alignment_log=os.path.join(output_dir_concat, "logs/mge/alignment_files.log")
    threads: rule_resources["create_alignment_log_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["create_alignment_log_concat"]["mem_mb"] * attempt
    retries: 3
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")










# Sequential filtering rules (fasta_cleaner)
rule human_cox1_filter_merge:
    input:
        alignment_log=os.path.join(output_dir_merge, "logs/mge/alignment_files.log")
    output:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        metrics=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/human_cox1_filter.py"),
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered"),
        human_threshold=fasta_cleaner_params["human_threshold"]
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner/01_human_filter.log")
    threads: rule_resources["human_cox1_filter_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["human_cox1_filter_merge"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-log {input.alignment_log} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --metrics-csv {output.metrics} \\
            --human-threshold {params.human_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """

rule at_content_filter_merge:
    input:
        human_filtered=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        summary_metrics=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/at_content_filter.py"),
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered"),
        at_threshold=fasta_cleaner_params["at_difference"],
        at_mode=fasta_cleaner_params["at_mode"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner/02_at_filter.log")
    threads: rule_resources["at_content_filter_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["at_content_filter_merge"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python -u {params.script} \\
            --input_files {input.human_filtered} \\
            --output_dir {params.output_dir} \\
	        --filtered-files-list {output.filtered_files} \\
            --at_threshold {params.at_threshold} \\
            --at_mode {params.at_mode} \\
            --consensus_threshold {params.consensus_threshold} \\
            --threads {threads} \\
	        2>&1 | tee {log}
        """

rule statistical_outlier_filter_merge:
    input:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        summary_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        individual_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/statistical_outlier_filter.py"),
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered"),
        outlier_percentile=fasta_cleaner_params["outlier_percentile"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner/03_outlier_filter.log")
    threads: rule_resources["statistical_outlier_filter_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["statistical_outlier_filter_merge"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-files-list {input.filtered_files} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --summary-csv {output.individual_metrics} \\
            --metrics-csv {output.summary_metrics} \\
            --outlier-percentile {params.outlier_percentile} \\
            --consensus-threshold {params.consensus_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """

if use_reference_filtering:
    # Version WITH reference filtering
    rule reference_filter_merge:
        input:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filtered.txt"),
            metrics=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/reference_filter.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered"),
            reference_dir=reference_dir,
            outlier_percentile=fasta_cleaner_params["outlier_percentile"],
            consensus_threshold=fasta_cleaner_params["consensus_threshold"]
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/04_reference_filter.log")
        threads: rule_resources["reference_filter_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["reference_filter_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail
    
            mkdir -p {params.output_dir}
    
            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --filtered-files-list {output.filtered_files} \\
                --metrics-csv {output.metrics} \\
                --reference-dir {params.reference_dir} \\
                --outlier-percentile {params.outlier_percentile} \\
                --consensus-threshold {params.consensus_threshold} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_merge:
        input:
            human_metrics=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            reference_metrics=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        output:
            completion_flag=os.path.join(output_dir_merge, "logs/cleaning_complete.txt"),
            combined_statistics=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner")
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --reference-metrics {input.reference_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """

else:
    # Version WITHOUT reference filtering
    rule consensus_generation_merge:
        input:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_merge, "fasta_cleaner/all_consensus_sequences.fasta"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/consensus_generator.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="merge"
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_merge:
        input:
            human_metrics=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        output:
            completion_flag=os.path.join(output_dir_merge, "logs/cleaning_complete.txt"),
            combined_statistics=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner")
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """


# Sequential filtering rules (fasta_cleaner)
rule human_cox1_filter_concat:
    input:
        alignment_log=os.path.join(output_dir_concat, "logs/mge/alignment_files.log")
    output:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        metrics=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/human_cox1_filter.py"),
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered"),
        human_threshold=fasta_cleaner_params["human_threshold"]
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner/01_human_filter.log")
    threads: rule_resources["human_cox1_filter_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["human_cox1_filter_concat"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-log {input.alignment_log} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --metrics-csv {output.metrics} \\
            --human-threshold {params.human_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """

rule at_content_filter_concat:
    input:
        human_filtered=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        summary_metrics=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/at_content_filter.py"),
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered"),
        at_threshold=fasta_cleaner_params["at_difference"],
        at_mode=fasta_cleaner_params["at_mode"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner/02_at_filter.log")
    threads: rule_resources["at_content_filter_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["at_content_filter_concat"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python -u {params.script} \\
            --input_files {input.human_filtered} \\
            --output_dir {params.output_dir} \\
	        --filtered-files-list {output.filtered_files} \\
            --at_threshold {params.at_threshold} \\
            --at_mode {params.at_mode} \\
            --consensus_threshold {params.consensus_threshold} \\
            --threads {threads} \\
	        2>&1 | tee {log}
        """

rule statistical_outlier_filter_concat:
    input:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        summary_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        individual_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/statistical_outlier_filter.py"),
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered"),
        outlier_percentile=fasta_cleaner_params["outlier_percentile"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner/03_outlier_filter.log")
    threads: rule_resources["statistical_outlier_filter_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["statistical_outlier_filter_concat"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-files-list {input.filtered_files} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --summary-csv {output.individual_metrics} \\
            --metrics-csv {output.summary_metrics} \\
            --outlier-percentile {params.outlier_percentile} \\
            --consensus-threshold {params.consensus_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """
		
if use_reference_filtering:
    # Version WITH reference filtering
    rule reference_filter_concat:
        input:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filtered.txt"),
            metrics=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/reference_filter.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered"),
            reference_dir=reference_dir,
            outlier_percentile=fasta_cleaner_params["outlier_percentile"],
            consensus_threshold=fasta_cleaner_params["consensus_threshold"]
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/04_reference_filter.log")
        threads: rule_resources["reference_filter_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["reference_filter_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail
            
            mkdir -p {params.output_dir}
            
            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --filtered-files-list {output.filtered_files} \\
                --metrics-csv {output.metrics} \\
                --reference-dir {params.reference_dir} \\
                --outlier-percentile {params.outlier_percentile} \\
                --consensus-threshold {params.consensus_threshold} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule consensus_generation_concat:
        input:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/consensus_generator.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="concat"
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_concat:
        input:
            human_metrics=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            reference_metrics=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        output:
            completion_flag=os.path.join(output_dir_concat, "logs/cleaning_complete.txt"),
            combined_statistics=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner")
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --reference-metrics {input.reference_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """

else:
    # Version WITHOUT reference filtering
    rule consensus_generation_concat:
        input:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/consensus_generator.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="concat"
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_concat:
        input:
            human_metrics=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/consensus_metrics.csv")
        output:
            completion_flag=os.path.join(output_dir_concat, "logs/cleaning_complete.txt"),
            combined_statistics=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner")
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """


# Clean up intermediate fasta files from filtering directories
rule remove_fasta_cleaner_files:
    input:
        # Wait for filtering to complete
        merge_cleaning_complete=os.path.join(output_dir_merge, "logs/cleaning_complete.txt"),
        concat_cleaning_complete=os.path.join(output_dir_concat, "logs/cleaning_complete.txt"),
        # Ensure final consensus sequences are created before cleanup
        merge_consensus=os.path.join(output_dir_merge, "fasta_cleaner/all_consensus_sequences.fasta"),
        concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta")
    output:
        merge_cleanup_complete=os.path.join(output_dir_merge, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        concat_cleanup_complete=os.path.join(output_dir_concat, "logs/fasta_cleaner/fasta_cleaner_complete.txt")
    params:
        merge_dir=output_dir_merge,
        concat_dir=output_dir_concat,
        use_ref_filtering=str(use_reference_filtering)
    threads: rule_resources["remove_fasta_cleaner_files"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["remove_fasta_cleaner_files"]["mem_mb"] * attempt
    retries: 2
    run:
        def cleanup_fasta_files(mode_dir, mode_name, completion_file):       
            removed_files = 0
            total_size_saved = 0
            
            with open(completion_file, 'w') as log:
                log.write(f"Starting intermediate fasta cleanup for {mode_name} mode: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                log.write(f"Mode directory: {mode_dir}\n")
                log.write(f"Reference filtering enabled: {params.use_ref_filtering}\n\n")
                
                # Define directories to clean
                filter_dirs = [
                    "01_human_filtered",
                    "02_at_filtered", 
                    "03_outlier_filtered"
                ]
                
                # Add reference filtering directory if enabled
                if params.use_ref_filtering == "True":
                    filter_dirs.append("04_reference_filtered")
                    log.write("Including 04_reference_filtered directory in cleanup\n")
                else:
                    log.write("Skipping 04_reference_filtered directory (not enabled)\n")
                
                log.write(f"Directories to clean: {', '.join(filter_dirs)}\n\n")
                
                # Clean each filtering directory
                for filter_dir in filter_dirs:
                    dir_path = os.path.join(mode_dir, "fasta_cleaner", filter_dir)
                    
                    if not os.path.exists(dir_path):
                        log.write(f"[{filter_dir}] Directory not found: {dir_path}\n")
                        continue
                    
                    log.write(f"[{filter_dir}] Cleaning directory: {dir_path}\n")
                    
                    # Find all .fasta and .fas files
                    fasta_patterns = [
                        os.path.join(dir_path, "*.fasta"),
                        os.path.join(dir_path, "*.fas"),
                        os.path.join(dir_path, "**", "*.fasta"),
                        os.path.join(dir_path, "**", "*.fas")
                    ]
                    
                    fasta_files = []
                    for pattern in fasta_patterns:
                        fasta_files.extend(glob.glob(pattern, recursive=True))
                    
                    # Remove duplicates
                    fasta_files = list(set(fasta_files))
                    
                    if not fasta_files:
                        log.write(f"[{filter_dir}] No .fasta or .fas files found\n")
                        continue
                    
                    log.write(f"[{filter_dir}] Found {len(fasta_files)} fasta files to remove:\n")
                    
                    dir_removed = 0
                    dir_size_saved = 0
                    
                    for fasta_file in fasta_files:
                        try:
                            # Get file size before removal
                            file_size = os.path.getsize(fasta_file)
                            
                            # Remove the file
                            os.remove(fasta_file)
                            
                            # Log the removal
                            size_mb = file_size / (1024 * 1024)
                            log.write(f"  - Removed: {os.path.basename(fasta_file)} ({size_mb:.2f} MB)\n")
                            
                            dir_removed += 1
                            dir_size_saved += file_size
                            
                        except OSError as e:
                            log.write(f"  - ERROR removing {fasta_file}: {e}\n")
                    
                    removed_files += dir_removed
                    total_size_saved += dir_size_saved
                    
                    size_saved_mb = dir_size_saved / (1024 * 1024)
                    log.write(f"[{filter_dir}] Removed {dir_removed} files, saved {size_saved_mb:.2f} MB\n\n")
                
                # Summary
                total_saved_mb = total_size_saved / (1024 * 1024)
                log.write(f"CLEANUP SUMMARY for {mode_name} mode:\n")
                log.write(f"  Total files removed: {removed_files}\n")
                log.write(f"  Total disk space saved: {total_saved_mb:.2f} MB\n")
                log.write(f"  Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                
                return removed_files, total_saved_mb
        
        # Clean up merge mode - write directly to completion file
        merge_removed, merge_saved = cleanup_fasta_files(params.merge_dir, "merge", output.merge_cleanup_complete)
        
        # Clean up concat mode - write directly to completion file
        concat_removed, concat_saved = cleanup_fasta_files(params.concat_dir, "concat", output.concat_cleanup_complete)


# Run Fasta_compare script on MGE and fasta_cleaner concatenated consensus multi-FASTAs
rule fasta_compare:
    input:
        fasta_concat=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        fasta_merge=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        clean_fasta_concat=os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta"),
        clean_fasta_merge=os.path.join(output_dir_merge, "fasta_cleaner/all_consensus_sequences.fasta"),
        stats_concat=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        stats_merge=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
    output:
        csv=os.path.join(main_output_dir, f"fasta_compare/{run_name}_fasta_compare.csv"),
        full_fasta=os.path.join(main_output_dir, f"fasta_compare/{run_name}_full_sequences.fasta"),
        barcode_fasta=os.path.join(main_output_dir, f"fasta_compare/{run_name}_barcode_sequences.fasta")
    params:
        script=os.path.join(workflow.basedir, "scripts/fasta_compare.py"),
        target_marker=fasta_compare_params["target"],
        rank=fasta_compare_params.get("rank", 3),
        relaxed=fasta_compare_params.get("relaxed", False),
        verbosity=" ".join([
            "--verbose" if fasta_compare_params["verbose"] else "",
            "--relaxed" if fasta_compare_params.get("relaxed", False) else ""
        ]).strip()
    log:
        os.path.join(main_output_dir, f"fasta_compare/fasta_compare_{run_name}.log")
    threads: rule_resources["fasta_compare"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fasta_compare"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        python {params.script} \
            --output-csv {output.csv} \
            --output-fasta {output.full_fasta} \
            --output-barcode {output.barcode_fasta} \
            --input {input.fasta_concat} {input.fasta_merge} {input.clean_fasta_concat} {input.clean_fasta_merge} \
            --target {params.target_marker} \
            --rank {params.rank} \
            --log-file {log} \
            {params.verbosity}
        """


# Extract, aggregate and calculate stats from inputs for merge mode
rule extract_stats_to_csv_merge:
    input:
        alignment_log=os.path.join(output_dir_merge, "logs/mge/alignment_files.log"),
        out_files=expand(
            os.path.join(output_dir_merge, "out/{sample}_r_{r}_s_{s}_summary.out"),
            sample=list(samples.keys()),
            r=config["r"],
            s=config["s"]
        ),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        cleaning_csv=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        cleaner_complete=os.path.join(output_dir_merge, "logs/cleaning_complete.txt")
    output:
        stats=os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/mge_stats.py"),
        out_dir=os.path.join(output_dir_merge, "out/"),
        log_dir=os.path.join(output_dir_merge, "logs/mge")
    threads: rule_resources["extract_stats_to_csv_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["extract_stats_to_csv_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

	    # Run script
        python {params.script} -a {input.alignment_log} -o {output.stats} -od {params.out_dir} -c {input.cleaning_csv}

        # Relocate logs
        mv mge_stats.log {params.log_dir}/
        """

# Extract, aggregate and calculate stats from inputs for concat mode
rule extract_stats_to_csv_concat:
    input:
        alignment_log=os.path.join(output_dir_concat, "logs/mge/alignment_files.log"),
        out_files=expand(
            os.path.join(output_dir_concat, "out/{sample}_r_{r}_s_{s}_summary.out"),
            sample=list(samples.keys()),
            r=config["r"],
            s=config["s"]
        ),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        cleaning_csv=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        cleaner_complete=os.path.join(output_dir_concat, "logs/cleaning_complete.txt")
    output:
        stats=os.path.join(output_dir_concat, f"{run_name}_concat-stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/mge_stats.py"),
        out_dir=os.path.join(output_dir_concat, "out/"),
        log_dir=os.path.join(output_dir_concat, "logs/mge")
    threads: rule_resources["extract_stats_to_csv_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["extract_stats_to_csv_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

		# Run script
        python {params.script} -a {input.alignment_log} -o {output.stats} -od {params.out_dir} -c {input.cleaning_csv}

        # Relocate logs
        mv mge_stats.log {params.log_dir}/
        """


# Concatenate merge and concat stats CSV files with column alignment
rule combine_stats_files:
    input:
        merge_stats=os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv"),
        concat_stats=os.path.join(output_dir_concat, f"{run_name}_concat-stats.csv")
    output:
        combined_stats=os.path.join(main_output_dir, f"{run_name}_combined_stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/csv_combiner_mge.py")
    threads: rule_resources["combine_stats_files"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["combine_stats_files"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        python {params.script} \
            --input {input.merge_stats} {input.concat_stats} \
            --output {output.combined_stats}
        """	


# Final clean up superfluous files
rule cleanup_files:
    input:
        # Merge mode inputs
        merge_summary_csv=os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv"),
        clean_headers_log=os.path.join(output_dir_merge, "logs/clean_headers.log"),
        merge_exon_remove=os.path.join(output_dir_merge, "logs/exonerate_cleanup_complete.txt"),
        # Concat mode inputs
        concat_summary_csv=os.path.join(output_dir_concat, f"{run_name}_concat-stats.csv"),
        concat_logs=os.path.join(output_dir_concat, "logs/concat_reads.log"),
        trim_galore_logs=os.path.join(output_dir_concat, "logs/trim_galore.log"),
        concat_exon_remove=os.path.join(output_dir_concat, "logs/exonerate_cleanup_complete.txt"),
        # Wait for fasta cleanup to complete
        merge_fasta_cleanup=os.path.join(output_dir_merge, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        concat_fasta_cleanup=os.path.join(output_dir_concat, "logs/fasta_cleaner/fasta_cleaner_complete.txt")
    output:
        merge_complete=touch(os.path.join(output_dir_merge, "logs/file_cleanup_complete.txt")),
        concat_complete=touch(os.path.join(output_dir_concat, "logs/file_cleanup_complete.txt"))
    threads: rule_resources["cleanup_files"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["cleanup_files"]["mem_mb"] * attempt
    retries: 2
    run:      
        # Function to clean up a specific mode's files
        def cleanup_mode_files(mode_dir, mode_name):
            removed_files = 0
            removed_dirs = []
            cleaned_dirs = []
            
            # Remove empty .log files in mge subdir - both modes
            mge_log_dir = os.path.join(mode_dir, "logs/mge")
            if os.path.exists(mge_log_dir):
                # Find all .log files in subdirectories of mge/
                mge_log_files = glob.glob(os.path.join(mge_log_dir, "**", "*.log"), recursive=True)
                removed_empty_logs = 0
                skipped_nonempty_logs = 0
                
                for mge_log_file in mge_log_files:
                    # Check if the log file is empty (0 bytes)
                    if os.path.getsize(mge_log_file) == 0:
                        print(f"[{mode_name}] Removing empty MGE log file: {mge_log_file}")
                        os.remove(mge_log_file)
                        removed_empty_logs += 1
                    else:
                        print(f"[{mode_name}] Keeping non-empty MGE log file: {mge_log_file} ({os.path.getsize(mge_log_file)} bytes)")
                        skipped_nonempty_logs += 1
                
                print(f"[{mode_name}] Removed {removed_empty_logs} empty MGE log files")
                print(f"[{mode_name}] Kept {skipped_nonempty_logs} non-empty MGE log files")
                removed_files += removed_empty_logs
                
                cleaned_dirs.append(mge_log_dir)
            else:
                print(f"[{mode_name}] MGE log directory not found: {mge_log_dir}")
            
            # Remove rename_complete.txt - both modes
            rename_complete_file = os.path.join(mode_dir, "logs/rename_complete.txt")
            if os.path.exists(rename_complete_file):
                print(f"[{mode_name}] Removing rename_complete.txt file: {rename_complete_file}")
                os.remove(rename_complete_file)
                removed_files += 1
            else:
                print(f"[{mode_name}] rename_complete.txt file not found: {rename_complete_file}")
            
            # Mode-specific cleanup
            if mode_name == "merge":
                # Remove aggregate_clean_headers.out file
                aggregate_clean_headers_out = os.path.join(mode_dir, "logs/aggregate_clean_headers.out")
                if os.path.exists(aggregate_clean_headers_out):
                    print(f"[{mode_name}] Removing aggregate_clean_headers.out file: {aggregate_clean_headers_out}")
                    os.remove(aggregate_clean_headers_out)
                    removed_files += 1
                else:
                    print(f"[{mode_name}] aggregate_clean_headers.out file not found: {aggregate_clean_headers_out}")
            
            elif mode_name == "concat":
                # Remove aggregate_concat.out file
                aggregate_concat_out = os.path.join(mode_dir, "logs/aggregate_concat.out")
                if os.path.exists(aggregate_concat_out):
                    print(f"[{mode_name}] Removing aggregate_concat.out file: {aggregate_concat_out}")
                    os.remove(aggregate_concat_out)
                    removed_files += 1
                else:
                    print(f"[{mode_name}] aggregate_concat.out file not found: {aggregate_concat_out}")
                
                # Remove aggregate_trim_galore.out file
                aggregate_trim_galore_out = os.path.join(mode_dir, "logs/aggregate_trim_galore.out")
                if os.path.exists(aggregate_trim_galore_out):
                    print(f"[{mode_name}] Removing aggregate_trim_galore.out file: {aggregate_trim_galore_out}")
                    os.remove(aggregate_trim_galore_out)
                    removed_files += 1
                else:
                    print(f"[{mode_name}] aggregate_trim_galore.out file not found: {aggregate_trim_galore_out}")
            
            # Return cleanup statistics
            return {
                "removed_files": removed_files,
                "removed_dirs": removed_dirs,
                "cleaned_dirs": cleaned_dirs
            }
        
        # Cleanup for merge mode
        merge_stats = cleanup_mode_files(output_dir_merge, "merge")
        
        # Cleanup for concat mode
        concat_stats = cleanup_mode_files(output_dir_concat, "concat")
        
        # Write completion message and cleanup info to merge output file
        with open(output.merge_complete, 'w') as f:
            f.write("Cleanup complete at " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            f.write(f"Total log files removed: {merge_stats['removed_files']}\n")            
            f.write("Directories fully removed:\n")
            if merge_stats['removed_dirs']:
                for dir_path in merge_stats['removed_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were fully removed\n")
            
            f.write("\nDirectories cleaned of log files:\n")
            if merge_stats['cleaned_dirs']:
                for dir_path in merge_stats['cleaned_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were cleaned\n")
            
            f.write("\nPreprocessing mode: merge")
        
        # Write completion message and cleanup info to concat output file
        with open(output.concat_complete, 'w') as f:
            f.write("Cleanup complete at " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            f.write(f"Total log files removed: {concat_stats['removed_files']}\n")
            
            f.write("Directories fully removed:\n")
            if concat_stats['removed_dirs']:
                for dir_path in concat_stats['removed_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were fully removed\n")
            
            f.write("\nDirectories cleaned of log files:\n")
            if concat_stats['cleaned_dirs']:
                for dir_path in concat_stats['cleaned_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were cleaned\n")
            
            f.write("\nPreprocessing mode: concat")
