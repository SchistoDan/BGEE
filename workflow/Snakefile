import os
import time
import csv
import yaml
import shutil
from snakemake.io import expand
import pandas as pd
import glob
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from pathlib import Path


# ===== CONFIGURATION AND VALIDATION =====

# Config validation
def validate_config(config):
    required_keys = ['samples_file', 'output_dir', 'r', 's', 'run_name', 'mge_path']
    missing_keys = [key for key in required_keys if key not in config]
    
    if missing_keys:
        sys.stderr.write(f"WARNING: Missing required configuration keys: {', '.join(missing_keys)}\n")
        sys.exit(1)
    
    # Validate file existence
    if not os.path.exists(config['samples_file']):
        sys.stderr.write(f"WARNING: Samples file not found: {config['samples_file']}\n")
        sys.exit(1)
    
    if not os.path.exists(config['mge_path']):
        sys.stderr.write(f"WARNING: MitoGeneExtractor path not found: {config['mge_path']}\n")
        sys.exit(1)
    
    # Validate gene_fetch configuration
    run_gene_fetch = config.get("run_gene_fetch", False)
    if not isinstance(run_gene_fetch, bool):
        sys.stderr.write("WARNING: 'run_gene_fetch' must be a boolean (true/false)\n")
        sys.exit(1)
    
    if run_gene_fetch:
        # Validate gene_fetch parameters
        gene_fetch_config = config.get("gene_fetch", {})
        required_gene_fetch_keys = ['email', 'api_key', 'gene', 'input_type']
        missing_gene_fetch_keys = [key for key in required_gene_fetch_keys if key not in gene_fetch_config or not gene_fetch_config[key]]
        
        if missing_gene_fetch_keys:
            sys.stderr.write(f"WARNING: Missing required gene_fetch configuration parameters: {', '.join(missing_gene_fetch_keys)}\n")
            sys.exit(1)
            
        # Validate input_type value
        input_type = gene_fetch_config.get("input_type", "").lower()
        if input_type not in ["taxid", "hierarchical"]:
            sys.stderr.write("WARNING: 'input_type' must be either 'taxid' or 'hierarchical'\n")
            sys.exit(1)
            
        # Validate samples_file exists
        samples_file = gene_fetch_config.get("samples_file", config.get("samples_file", ""))
        if not samples_file or not os.path.exists(samples_file):
            sys.stderr.write(f"WARNING: Gene fetch samples file not found: {samples_file}\n")
            sys.exit(1)
    else:
        # If not using gene_fetch, validate sequence_reference_file exists
        if 'sequence_reference_file' not in config:
            sys.stderr.write("WARNING: 'sequence_reference_file' is required when run_gene_fetch is false\n")
            sys.exit(1)
        
        if not os.path.exists(config['sequence_reference_file']):
            sys.stderr.write(f"WARNING: Sequence reference file not found: {config['sequence_reference_file']}\n")
            sys.exit(1)
    
    # Validate r and s parameters
    if not isinstance(config['r'], list) or not config['r']:
        sys.stderr.write("WARNING: 'r' must be a non-empty list\n")
        sys.exit(1)
    
    if not isinstance(config['s'], list) or not config['s']:
        sys.stderr.write("WARNING: 's' must be a non-empty list\n")
        sys.exit(1)
		
    # Validate run_fasta_compare parameter
    run_fasta_compare = config.get("run_fasta_compare", True)  # Default to True
    if not isinstance(run_fasta_compare, bool):
        sys.stderr.write("WARNING: 'run_fasta_compare' must be a boolean (true/false)\n")
        sys.exit(1)

# Validate configuration file
validate_config(config)

# Print configuration for debugging
print("\n=====Configuration loaded:=====\n", config) 




# ===== DATA PARSING FUNCTIONS =====

# Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples

# Parse references from CSV
def parse_sequence_references(sequence_reference_file):
    sequence_references = {}
    with open(sequence_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            protein_reference_path = row['protein_reference_path']
            nucleotide_reference_path = row['nucleotide_reference_path']
            sequence_references[sample_id] = {
                "reference_name": reference_name, 
                "protein_path": protein_reference_path,
                "nucleotide_path": nucleotide_reference_path
            }    
    return sequence_references



# ===== PARAMETER SETUP & CONFIG PARSING =====

# Load parameters from config file
samples = parse_samples(config["samples_file"])
run_name = config["run_name"]

r = config["r"]
s = config["s"]
n = config["n"]
C = config["C"]
t = config["t"]
mge_path = config["mge_path"]


# Determine sequence reference file path based on gene_fetch setting
run_gene_fetch = config.get("run_gene_fetch", False) # Default to False
if run_gene_fetch:
    sequence_reference_file_path = os.path.join(config["output_dir"], "references", "sequence_references.csv")
else:
    sequence_reference_file_path = config["sequence_reference_file"]

# Load downsampling parameters
downsampling_enabled = config.get("downsampling", {}).get("enabled", False)
max_reads = config.get("downsampling", {}).get("max_reads", 0)

# Load fasta_cleaner parameters
fasta_cleaner_params = config.get("fasta_cleaner", {	
    "consensus_threshold": 0.5,
    "human_threshold": 0.95,
    "at_difference": 0.1,
    "at_mode": "absolute",
    "outlier_percentile": 90.0,
    "disable_human": False,
    "disable_at": False,
    "disable_outliers": False
})

# Check if fasta_compare should be performed
run_fasta_compare = config.get("run_fasta_compare", True)  # Default to True

# Load fasta_compare parameters
fasta_compare_params = config.get("fasta_compare", {
    "target": "cox1",
    "verbose": False
})

# Load rule resources 
rule_resources = config["rules"]

# Create main output directory
main_output_dir = config["output_dir"]
Path(main_output_dir).mkdir(parents=True, exist_ok=True)

# Create separate output directories for each preprocessing mode
output_dir_merge = os.path.join(main_output_dir, "merge_mode")
output_dir_concat = os.path.join(main_output_dir, "concat_mode")

# Create references directory if using gene_fetch
if run_gene_fetch:
    references_dir = os.path.join(main_output_dir, "references")
    Path(references_dir).mkdir(parents=True, exist_ok=True)

# Check if reference filtering should be performed
reference_dir = fasta_cleaner_params.get("reference_dir", None)
use_reference_filtering = reference_dir and reference_dir not in ["None", "null", "", None]



# ===== HELPER FUNCTIONS =====

# Handles sequence_reference_file dependency for MGE and file path
def get_protein_reference(wildcards):
    """Get protein reference path for a sample"""
    if run_gene_fetch:
        # Don't read the CSV - construct the path directly
        # Assumes gene_fetch creates files like: references/{sample}_protein.faa
        return os.path.join(main_output_dir, f"references/protein/{wildcards.sample}.fasta")
    else:
        # For manual references, we can read the file immediately since it exists
        sequence_refs = parse_sequence_references(config["sequence_reference_file"])
        return sequence_refs[wildcards.sample]["protein_path"]
	
# Generate all combinations for each sample+reference pair with all r and s values
def get_all_mge_consensus_files(mode):
    """Generate list of expected consensus files for a given mode"""
    output_dir = output_dir_merge if mode == "merge" else output_dir_concat
    files = []
    
    for sample in samples.keys():
        for r_val in r:
            for s_val in s:
                files.append(os.path.join(output_dir, f"consensus/{sample}_r_{r_val}_s_{s_val}_con_{sample}.fas"))
    
    return files

# Get the right input for merge mode
def get_mge_input_merge(wildcards):
    return os.path.join(output_dir_merge, f"trimmed_data/{wildcards.sample}_merged_clean.fq")

# Get input for concat mode
def get_mge_input_concat(wildcards):
    return os.path.join(output_dir_concat, f"trimmed_data/{wildcards.sample}/{wildcards.sample}_concat_trimmed.fq")



# ===== SNAKEMAKE RULES =====

# Rule them all
rule all:
    input:
        # ==== GENE FETCH OUTPUT (CONDITIONAL) ====
        (os.path.join(main_output_dir, "references/sequence_references.csv") if run_gene_fetch else []),
        
        # ==== MERGE MODE OUTPUTS ====
        os.path.join(output_dir_merge, "logs/clean_headers/clean_headers.log"),
        os.path.join(output_dir_merge, f"consensus/{run_name}_cons_combined-merge.fasta"),
        os.path.join(output_dir_merge, "logs/mge/alignment_files.log"),
        os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv"),
        os.path.join(output_dir_merge, "logs/fasta_cleaner_complete.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        os.path.join(output_dir_merge, "logs/final_cleanup_complete.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
        os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
        os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        (os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filtered.txt") if use_reference_filtering else []),
        (os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv") if use_reference_filtering else []), 
        os.path.join(output_dir_merge, "fasta_cleaner/cleaned_cons_combined.fasta"),
        os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-merge.csv"),
        os.path.join(output_dir_merge, "logs/exonerate_int_cleanup_complete.txt"),
        
        # ==== CONCAT MODE OUTPUTS ====
        os.path.join(output_dir_concat, "logs/concat/concat_reads.log"),
        os.path.join(output_dir_concat, "logs/trim_galore/trim_galore.log"),
        expand(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq.gz"), sample=list(samples.keys())),
        expand(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq.gz"), sample=list(samples.keys())), 
        os.path.join(output_dir_concat, f"consensus/{run_name}_cons_combined-concat.fasta"),
        os.path.join(output_dir_concat, "logs/mge/alignment_files.log"),
        os.path.join(output_dir_concat, f"{run_name}_cons_combined-concat-stats.csv"),
        os.path.join(output_dir_concat, "logs/fasta_cleaner_complete.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
        os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
        os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        (os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filtered.txt") if use_reference_filtering else []),
        (os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv") if use_reference_filtering else []),
        os.path.join(output_dir_concat, "fasta_cleaner/cleaned_cons_combined.fasta"),
        os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-concat.csv"),
        os.path.join(output_dir_concat, "logs/exonerate_int_cleanup_complete.txt"),
		
        # ==== FASTA COMPARE OUTPUTS (CONDITIONAL) ====
        (os.path.join(main_output_dir, f"fasta_compare/{run_name}_fasta_compare.csv") if run_fasta_compare else []),
        (os.path.join(main_output_dir, f"fasta_compare/{run_name}_full_sequences.fasta") if run_fasta_compare else []),
        (os.path.join(main_output_dir, f"fasta_compare/{run_name}_barcode_sequences.fasta") if run_fasta_compare else []),
		
        # ==== FINAL OUTPUTS ====
        os.path.join(main_output_dir, f"{run_name}_combined_stats.csv"),
        os.path.join(output_dir_merge, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        os.path.join(output_dir_concat, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        os.path.join(output_dir_concat, "logs/final_cleanup_complete.txt"),


# ===== GENE FETCH RULE =====
if run_gene_fetch:
    rule gene_fetch:
        input:
            samples_csv=lambda wildcards: config["gene_fetch"].get("samples_file", config["samples_file"])
        output:
            sequence_references=os.path.join(main_output_dir, "references/sequence_references.csv"),
            protein_files=expand(
                os.path.join(main_output_dir, "references/protein/{sample}.fasta"),
                sample=list(samples.keys())
            )
        params:
            email=config["gene_fetch"]["email"],
            api_key=config["gene_fetch"]["api_key"],
            gene=config["gene_fetch"]["gene"],
            input_type=config["gene_fetch"]["input_type"].lower(),
            genbank=config["gene_fetch"].get("genbank", False),
            output_dir=os.path.join(main_output_dir, "references"),
            genbank_flag="--genbank" if config["gene_fetch"].get("genbank", False) else "",
            input_flag="--in" if config["gene_fetch"]["input_type"].lower() == "taxid" else "--in2"
        log:
            os.path.join(main_output_dir, "references/gene_fetch.log")
        threads: rule_resources["gene_fetch"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["gene_fetch"]["mem_mb"] * attempt
        retries: 3
        shell:
            """
            set -euo pipefail
            
            echo "Starting gene-fetch: $(date)" > {log}
            echo "Email: {params.email}" >> {log}
            echo "Gene: {params.gene}" >> {log}
            echo "Type: protein" >> {log}
            echo "Input type: {params.input_type}" >> {log}
            echo "Genbank: {params.genbank}" >> {log}
            echo "Input samples: {input.samples_csv}" >> {log}
            echo "Output directory: {params.output_dir}" >> {log}
            
            # Run gene-fetch
            gene-fetch \\
                --email {params.email} \\
                --api-key {params.api_key} \\
                {params.input_flag} {input.samples_csv} \\
                --gene {params.gene} \\
                --type protein \\
                {params.genbank_flag} \\
                --out {params.output_dir} \\
                >> {log} 2>&1
            
            echo "Gene-fetch completed: $(date)" >> {log}
            echo "Output file: {output.sequence_references}" >> {log}
            
            # Verify output file was created
            if [ ! -f "{output.sequence_references}" ]; then
                echo "ERROR: Expected output file not found: {output.sequence_references}" >> {log}
                exit 1
            fi
            """


# ===== MERGE MODE RULES =====
rule fastp_pe_merge:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_R1_trimmed.fq.gz")),
        R2_trimmed=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_R2_trimmed.fq.gz")),
        report=os.path.join(output_dir_merge, "trimmed_data/{sample}_fastp_report.html"),
        json=os.path.join(output_dir_merge, "trimmed_data/{sample}_fastp_report.json"),
        merged_reads=os.path.join(output_dir_merge, "trimmed_data/{sample}_merged.fq"),
        unpaired_R1=temp(os.path.join(output_dir_merge, "trimmed_data/unpaired/{sample}_unpaired_R1.fq")),
        unpaired_R2=temp(os.path.join(output_dir_merge, "trimmed_data/unpaired/{sample}_unpaired_R2.fq"))
    params:
        reads_to_process=max_reads if downsampling_enabled else 0
    log:
        out=os.path.join(output_dir_merge, "logs/fastp/{sample}_fastp.out"),
        err=os.path.join(output_dir_merge, "logs/fastp/{sample}_fastp.err")
    threads: rule_resources["fastp_pe_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fastp_pe_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
		
        # Log job start
        echo "======== Fastp Merge Processing ========" > {log.out}
        echo "Sample: {wildcards.sample}" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Input files: {input.R1}, {input.R2}" >> {log.out}
        echo "Downsampling: --reads_to_process {params.reads_to_process}" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "=========================================" >> {log.out}

        # Check input files exist
        if [[ ! -f "{input.R1}" ]] || [[ ! -f "{input.R2}" ]]; then
            echo "ERROR: Input files not found" >> {log.out}
            exit 1
        fi

        # Log input file sizes
        echo "Input file sizes:" >> {log.out}
        echo "  R1: $(numfmt --to=iec $(stat -c%s {input.R1}))" >> {log.out}
        echo "  R2: $(numfmt --to=iec $(stat -c%s {input.R2}))" >> {log.out}

        # Run fastp
        echo "Running fastp: $(date)" >> {log.out}
        if fastp -i {input.R1} -I {input.R2} \\
              -o {output.R1_trimmed} -O {output.R2_trimmed} \\
              --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \\
              --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \\
              --dedup \\
              --trim_poly_g \\
              --thread {threads} \\
              --merge --merged_out {output.merged_reads} \\
              --unpaired1 {output.unpaired_R1} \\
              --unpaired2 {output.unpaired_R2} \\
              --reads_to_process {params.reads_to_process} \\
              -h {output.report} -j {output.json} \\
              >> {log.out} 2>> {log.err}; then
            echo "Fastp completed successfully: $(date)" >> {log.out}
        else
            echo "ERROR: Fastp failed with exit code $?" >> {log.out}
            exit 1
        fi

        # Log output file sizes
        echo "Output file sizes:" >> {log.out}
        echo "  R1_trimmed: $(numfmt --to=iec $(stat -c%s {output.R1_trimmed}))" >> {log.out}
        echo "  R2_trimmed: $(numfmt --to=iec $(stat -c%s {output.R2_trimmed}))" >> {log.out}
        echo "  Merged: $(numfmt --to=iec $(stat -c%s {output.merged_reads}))" >> {log.out}
        echo "Processing completed: $(date)" >> {log.out}
        """

rule clean_headers_merge:
    input:
        merged_reads=os.path.join(output_dir_merge, "trimmed_data/{sample}_merged.fq")
    output:
        clean_merged=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_merged_clean.fq"))
    log:
        out=temp(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.out")),
        err=temp(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.err"))
    threads: rule_resources["clean_headers_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["clean_headers_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        # Create directories
        mkdir -p $(dirname {log.out})
        mkdir -p $(dirname {output.clean_merged})

        # Create log header
        echo "Cleaning headers for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Host: $(hostname)" >> {log.out}

        # Simple sed replacement with error handling
        if sed 's/ /_/g' {input.merged_reads} > {output.clean_merged} 2>> {log.err}; then
            echo "Completed: $(date)" >> {log.out}
            echo "Output file size: $(numfmt --to=iec $(stat -c%s {output.clean_merged}))" >> {log.out}
            echo "--------------------------------------------" >> {log.out}
        else
            echo "FAILED: $(date)" >> {log.out}
            exit 1
        fi
        """

rule aggregate_clean_headers_logs_merge:
    input:
        sample_logs=expand(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.out"), sample=list(samples.keys()))
    output:
        combined_log=os.path.join(output_dir_merge, "logs/clean_headers/clean_headers.log") 
    log:
        out=os.path.join(output_dir_merge, "logs/clean_headers/aggregate_clean_headers.out"),
        err=os.path.join(output_dir_merge, "logs/clean_headers/aggregate_clean_headers.err")
    threads: rule_resources["aggregate_clean_headers_logs_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["aggregate_clean_headers_logs_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        # Create header for aggregated log file
        echo "===============================================" > {output.combined_log}
        echo "Aggregated clean headers logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}

        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log} 2>> {log.err}
        echo "Aggregation complete." >> {log.out}
        """

# Helper function to get the right input for merge mode
def get_mge_input_merge(wildcards):
    return os.path.join(output_dir_merge, f"trimmed_data/{wildcards.sample}_merged_clean.fq")


# ===== CONCAT MODE RULES =====
rule fastp_pe_concat:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq"),
        R2_trimmed=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq"),
        report=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_fastp_report.html"),
        json=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_fastp_report.json")
    params:
        reads_to_process=max_reads if downsampling_enabled else 0
    log:
        out=os.path.join(output_dir_concat, "logs/fastp/{sample}_fastp.out"),
        err=os.path.join(output_dir_concat, "logs/fastp/{sample}_fastp.err")
    threads: rule_resources["fastp_pe_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fastp_pe_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        # Create output directories
        mkdir -p $(dirname {output.R1_trimmed})
        mkdir -p $(dirname {output.report})
        
        # Log job start
        echo "======== Fastp Concat Processing ========" > {log.out}
        echo "Sample: {wildcards.sample}" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Input files: {input.R1}, {input.R2}" >> {log.out}
        echo "Downsampling: --reads_to_process {params.reads_to_process}" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "=========================================" >> {log.out}
        
        # Check input files exist
        if [[ ! -f "{input.R1}" ]] || [[ ! -f "{input.R2}" ]]; then
            echo "ERROR: Input files not found" >> {log.out}
            exit 1
        fi

        # Log input file sizes
        echo "Input file sizes:" >> {log.out}
        echo "  R1: $(numfmt --to=iec $(stat -c%s {input.R1}))" >> {log.out}
        echo "  R2: $(numfmt --to=iec $(stat -c%s {input.R2}))" >> {log.out}
        
        # Run fastp
        echo "Running fastp: $(date)" >> {log.out}
        if fastp -i {input.R1} -I {input.R2} \\
                 -o {output.R1_trimmed} -O {output.R2_trimmed} \\
                 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \\
                 -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \\
                 --dedup \\
                 --trim_poly_g \\
                 --thread {threads} \\
                 --reads_to_process {params.reads_to_process} \\
                 -h {output.report} -j {output.json} \\
                 >> {log.out} 2>> {log.err}; then
            echo "Fastp completed successfully: $(date)" >> {log.out}
        else
            echo "ERROR: Fastp failed with exit code $?" >> {log.out}
            exit 1
        fi
        
        # Validate outputs exist
        if [[ ! -s {output.R1_trimmed} ]] || [[ ! -s {output.R2_trimmed} ]]; then
            echo "ERROR: Output files are empty or missing" >> {log.out}
            exit 1
        fi
        
        # Clean headers
        echo "Cleaning headers: $(date)" >> {log.out}
        perl -i -pe 's/ /_/g' {output.R1_trimmed}
        perl -i -pe 's/ /_/g' {output.R2_trimmed}
        
        # Log output file sizes
        echo "Output file sizes:" >> {log.out}
        echo "  R1_trimmed: $(numfmt --to=iec $(stat -c%s {output.R1_trimmed}))" >> {log.out}
        echo "  R2_trimmed: $(numfmt --to=iec $(stat -c%s {output.R2_trimmed}))" >> {log.out}
        echo "Processing completed: $(date)" >> {log.out}
        """

rule fastq_concat:
    input:
        R1=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq"),
        R2=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq")
    output:
        temp(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq"))
    log:
        out=os.path.join(output_dir_concat, "logs/concat/{sample}.out")
    threads: rule_resources["fastq_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fastq_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
                
        echo "Concatenating files for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}

        # Concatenation
        (cat {input.R1} && cat {input.R2}) > {output} 2>> {log.out}

        echo "Completed: $(date)" >> {log.out}
        echo "Output file size: $(numfmt --to=iec $(stat -c%s {output}))" >> {log.out}
        """


rule gzip_trimmed:
    input:
        R1=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq"),
        R2=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq"),
        concat=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq")
    output:
        R1_gz=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R1_trimmed.fastq.gz"),
        R2_gz=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_R2_trimmed.fastq.gz")
    log:
        out=os.path.join(output_dir_concat, "logs/gzip/{sample}.out")
    threads: rule_resources["gzip_trimmed"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["gzip_trimmed"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
        
        echo "Compressing trimmed files for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}

        # Compress the trimmed files using pigz for parallel compression
        echo "Compressing R1 file with pigz (using {threads} threads)..." >> {log.out}
        pigz -c -p {threads} {input.R1} > {output.R1_gz} 2>> {log.out}
        
        echo "Compressing R2 file with pigz (using {threads} threads)..." >> {log.out}
        pigz -c -p {threads} {input.R2} > {output.R2_gz} 2>> {log.out}

        # Remove the original uncompressed files
        echo "Removing original uncompressed files..." >> {log.out}
        rm {input.R1} {input.R2}

        echo "Compression completed: $(date)" >> {log.out}
        echo "Compressed file sizes:" >> {log.out}
        echo "  R1: $(numfmt --to=iec $(stat -c%s {output.R1_gz}))" >> {log.out}
        echo "  R2: $(numfmt --to=iec $(stat -c%s {output.R2_gz}))" >> {log.out}
        """

rule aggregate_concat_logs:
    input:
        sample_logs=expand(os.path.join(output_dir_concat, "logs/concat/{sample}.out"), sample=list(samples.keys()))
    output:
        combined_log=os.path.join(output_dir_concat, "logs/concat/concat_reads.log")
    log:
        out=os.path.join(output_dir_concat, "logs/concat/aggregate_concat.out"),
        err=os.path.join(output_dir_concat, "logs/concat/aggregate_concat.err")
    threads: rule_resources["aggregate_concat_logs"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["aggregate_concat_logs"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        echo "===============================================" > {output.combined_log}
        echo "Aggregated concat logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}

        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log} 2>> {log.err}
        echo "Aggregation complete." >> {log.out}
        """


rule quality_trim:
    input:
        os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq")
    output:
        temp(os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat_trimmed.fq")),
        report=os.path.join(output_dir_concat, "trimmed_data/{sample}/{sample}_concat.fastq_trimming_report.txt")
    params:
        outdir=os.path.join(output_dir_concat, "trimmed_data/{sample}"),
        report_dir=os.path.join(output_dir_concat, "trimmed_data/")
    log:
        out=os.path.join(output_dir_concat, "logs/trim_galore/{sample}.out"),
        err=os.path.join(output_dir_concat, "logs/trim_galore/{sample}.err")
    threads: rule_resources["quality_trim"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["quality_trim"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        echo "Quality trimming for {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Input file: {input}" >> {log.out}

        # Run trim_galore
        trim_galore --cores {threads} \\
                    --dont_gzip \\
                    --output_dir {params.outdir} \\
                    --basename {wildcards.sample}_concat \\
                    {input} >> {log.out} 2>> {log.err}

        # Move reports to out location
        #mv {params.outdir}/{wildcards.sample}_concat.fastq_trimming_report.txt {output.report} >> {log.out} 2>> {log.err}

        # Log job completion
        echo "Completed: $(date)" >> {log.out}
        echo "Output file size: $(numfmt --to=iec $(stat -c%s {output[0]}))" >> {log.out}
        """

rule aggregate_trim_galore_logs:
    input:
        sample_logs=expand(os.path.join(output_dir_concat, "logs/trim_galore/{sample}.out"), sample=list(samples.keys())) # Use .out
    output:
        combined_log=os.path.join(output_dir_concat, "logs/trim_galore/trim_galore.log")
    log:
        out=os.path.join(output_dir_concat, "logs/aggregate_trim_galore.out"),
        err=os.path.join(output_dir_concat, "logs/aggregate_trim_galore.err")
    threads: rule_resources["aggregate_trim_galore_logs"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["aggregate_trim_galore_logs"]["mem_mb"] * attempt
    shell:
        """
        set -euo pipefail

        echo "===============================================" > {output.combined_log}
        echo "Aggregated trim_galore logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}

        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log} 2>> {log.err}
        echo "Aggregation complete." >> {log.out}
        """

# Helper function to get the right input for concat mode
def get_mge_input_concat(wildcards):
    return os.path.join(output_dir_concat, f"trimmed_data/{wildcards.sample}/{wildcards.sample}_concat_trimmed.fq")





# ===== SHARED RULES WITH MODE-SPECIFIC VERSIONS =====

# MGE rule for merge mode
rule MitoGeneExtractor_merge:
    input:
        DNA=get_mge_input_merge,
        AA=get_protein_reference
    output:
        alignment=os.path.join(output_dir_merge, "alignment/{sample}_r_{r}_s_{s}_align_{sample}.fas"),
        consensus=os.path.join(output_dir_merge, "consensus/{sample}_r_{r}_s_{s}_con_{sample}.fas")
    log:
        out=os.path.join(output_dir_merge, "out/{sample}_r_{r}_s_{s}_summary.out"),
        err=os.path.join(output_dir_merge, "err/{sample}_r_{r}_s_{s}_summary.err")
    params:
        mge_executor=config["mge_path"],
        n=config["n"],
        C=config["C"],
        t=config["t"],		
        output_dir=output_dir_merge,
        vulgar_dir=lambda wildcards: os.path.join(output_dir_merge, f"logs/mge/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}/")
    threads: rule_resources["MitoGeneExtractor_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["MitoGeneExtractor_merge"]["mem_mb"] * attempt,
        slurm_partition=rule_resources["MitoGeneExtractor_merge"]["partition"]
    retries: 4
    shell:
        """
        set -euo pipefail

        # Check if input file is empty or missing
        if [ ! -s "{input.DNA}" ]; then
            echo "===== EMPTY/MISSING INPUT DETECTED =====" >> {log.out}
            echo "Started: $(date)" >> {log.out}
            echo "Sample: {wildcards.sample}" >> {log.out}
            echo "Mode: merge" >> {log.out}
            echo "Input file: {input.DNA}" >> {log.out}
            
            if [ ! -f "{input.DNA}" ]; then
                echo "Input file does not exist" >> {log.out}
            else
                FILE_SIZE=$(stat -c%s "{input.DNA}" 2>/dev/null || echo "unknown")
                echo "Input file size: $FILE_SIZE bytes" >> {log.out}
                echo "File is empty - likely all reads were filtered out during trimming" >> {log.out}
            fi
            
            echo "Creating mock outputs..." >> {log.out}
            echo ">Consensus__{wildcards.sample}" > {output.consensus}
            touch {output.alignment}
            touch {log.err}
            echo "Mock outputs created successfully: $(date)" >> {log.out}
            echo "=========================================" >> {log.out}
            exit 0
        fi

        # Create vulgar directory if it doesn't exist
        mkdir -p {params.vulgar_dir}
        
        # Job info and file analysis
        echo "===== Job Info =====" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Node: $(hostname)" >> {log.out}
        echo "Memory allocated: $(({resources.mem_mb} / 1024))GB" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "Mode: merge" >> {log.out}
        echo "Sample: {wildcards.sample}" >> {log.out}
        
        # Accurate file size analysis (using stat - the reliable method)
        FILE_SIZE_BYTES=$(stat -c%s "{input.DNA}")
        FILE_SIZE_GB=$(echo "scale=2; $FILE_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
        NUM_SEQUENCES=$(grep -c "^@" "{input.DNA}" || echo "0")
        
        echo "===== Input Files =====" >> {log.out}
        echo "Input file: {input.DNA}" >> {log.out}
        echo "File size: ${{FILE_SIZE_GB}}GB ($FILE_SIZE_BYTES bytes)" >> {log.out}
        echo "Number of sequences: $NUM_SEQUENCES" >> {log.out}        
        echo "Input AA file: {input.AA}" >> {log.out}
            
        # Run MitoGeneExtractor
        echo "===== Starting MitoGeneExtractor =====" >> {log.out}
        echo "Command: {params.mge_executor} -q {input.DNA} -p {input.AA} -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ -V {params.vulgar_dir} -r {wildcards.r} -s {wildcards.s} -n {params.n} -C {params.C} -t {params.t} --temporaryDirectory {params.output_dir} --verbosity 10" >> {log.out}
        echo "=======================================" >> {log.out}
        
        time {params.mge_executor} \\
        -q {input.DNA} -p {input.AA} \\
        -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \\
        -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \\
        -V {params.vulgar_dir} \\
        -r {wildcards.r} -s {wildcards.s} \\
        -n {params.n} \\
        -C {params.C} \\
        -t {params.t} \\
        --temporaryDirectory {params.output_dir} \\
        --verbosity 10 \\
        >> {log.out} 2>> {log.err} || {{
            EXIT_CODE=$?
            echo "===== MitoGeneExtractor Failed =====" >> {log.out}
            echo "Exit code: $EXIT_CODE" >> {log.out}
            echo "Finished: $(date)" >> {log.out}
            
            # Enhanced failure analysis
            if [ $EXIT_CODE -eq 137 ]; then
                echo "ANALYSIS: Process killed (likely out of memory)" >> {log.out}
                echo "  Current allocation: $(({resources.mem_mb} / 1024))GB" >> {log.out}
                echo "  File size: ${{FILE_SIZE_GB}}GB" >> {log.out}
                echo "  Sequences processed: $NUM_SEQUENCES" >> {log.out}
            elif [ $EXIT_CODE -eq 124 ]; then
                echo "ANALYSIS: Process timed out" >> {log.out}
            elif [ $EXIT_CODE -eq 1 ]; then
                echo "ANALYSIS: General error - check error log for details" >> {log.out}
            else
                echo "ANALYSIS: Unknown failure (exit code $EXIT_CODE)" >> {log.out}
            fi
            
            echo "Final system state:" >> {log.out}
            free -h >> {log.out}
            echo "===================================" >> {log.out}
            
            exit $EXIT_CODE
        }}

        echo "===== Job Completed Successfully =====" >> {log.out}
        echo "Finished: $(date)" >> {log.out}
        echo "Final memory usage:" >> {log.out}
        free -h >> {log.out}
        
        # Validate and report output files
        if [ -f "{output.consensus}" ] && [ -f "{output.alignment}" ]; then
            CONSENSUS_SIZE=$(stat -c%s {output.consensus})
            ALIGNMENT_SIZE=$(stat -c%s {output.alignment})
            CONSENSUS_LINES=$(wc -l < {output.consensus})
            
            echo "===== Output Summary =====" >> {log.out}
            echo "Consensus file: $(numfmt --to=iec $CONSENSUS_SIZE) ($CONSENSUS_LINES lines)" >> {log.out}
            echo "Alignment file: $(numfmt --to=iec $ALIGNMENT_SIZE)" >> {log.out}
            
            if [ $CONSENSUS_LINES -gt 1 ]; then
                echo "✓ Consensus contains sequence data" >> {log.out}
            else
                echo "⚠ WARNING: Consensus may be header-only" >> {log.out}
            fi
            echo "===========================" >> {log.out}
        else
            echo "ERROR: Expected output files not found" >> {log.out}
            echo "Expected files:" >> {log.out}
            echo "  Consensus: {output.consensus}" >> {log.out}
            echo "  Alignment: {output.alignment}" >> {log.out}
            exit 1
        fi
        """


# MGE rule for concat mode
rule MitoGeneExtractor_concat:
    input:
        DNA=get_mge_input_concat,
        AA=get_protein_reference
    output:
        alignment=os.path.join(output_dir_concat, "alignment/{sample}_r_{r}_s_{s}_align_{sample}.fas"),
        consensus=os.path.join(output_dir_concat, "consensus/{sample}_r_{r}_s_{s}_con_{sample}.fas")
    log:
        out=os.path.join(output_dir_concat, "out/{sample}_r_{r}_s_{s}_summary.out"),
        err=os.path.join(output_dir_concat, "err/{sample}_r_{r}_s_{s}_summary.err")
    params:
        mge_executor=config["mge_path"],
        n=config["n"],
        C=config["C"],
        t=config["t"],		
        output_dir=output_dir_concat,
        vulgar_dir=lambda wildcards: os.path.join(output_dir_concat, f"logs/mge/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}/")
    threads: rule_resources["MitoGeneExtractor_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["MitoGeneExtractor_concat"]["mem_mb"] * attempt,
        slurm_partition=rule_resources["MitoGeneExtractor_concat"]["partition"]
    retries: 4
    shell:
        """
        set -euo pipefail

        # Check if input file is empty or missing
        if [ ! -s "{input.DNA}" ]; then
            echo "===== EMPTY/MISSING INPUT DETECTED =====" >> {log.out}
            echo "Started: $(date)" >> {log.out}
            echo "Sample: {wildcards.sample}" >> {log.out}
            echo "Mode: concat" >> {log.out}
            echo "Input file: {input.DNA}" >> {log.out}
            
            if [ ! -f "{input.DNA}" ]; then
                echo "Input file does not exist" >> {log.out}
            else
                FILE_SIZE=$(stat -c%s "{input.DNA}" 2>/dev/null || echo "unknown")
                echo "Input file size: $FILE_SIZE bytes" >> {log.out}
                echo "File is empty - likely all reads were filtered out during trimming" >> {log.out}
            fi
            
            echo "Creating mock outputs..." >> {log.out}
            echo ">Consensus__{wildcards.sample}" > {output.consensus}
            touch {output.alignment}
            touch {log.err}
            echo "Mock outputs created successfully: $(date)" >> {log.out}
            echo "=========================================" >> {log.out}
            exit 0
        fi

        # Create vulgar directory if it doesn't exist
        mkdir -p {params.vulgar_dir}
        
        # Job info and file analysis
        echo "===== Job Info =====" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Node: $(hostname)" >> {log.out}
        echo "Memory allocated: $(({resources.mem_mb} / 1024))GB" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "Mode: concat" >> {log.out}
        echo "Sample: {wildcards.sample}" >> {log.out}
        
        # Accurate file size analysis (using stat - the reliable method)
        FILE_SIZE_BYTES=$(stat -c%s "{input.DNA}")
        FILE_SIZE_GB=$(echo "scale=2; $FILE_SIZE_BYTES / 1024 / 1024 / 1024" | bc)
        NUM_SEQUENCES=$(grep -c "^@" "{input.DNA}" || echo "0")
        
        echo "===== Input Files =====" >> {log.out}
        echo "Input file: {input.DNA}" >> {log.out}
        echo "File size: ${{FILE_SIZE_GB}}GB ($FILE_SIZE_BYTES bytes)" >> {log.out}
        echo "Number of sequences: $NUM_SEQUENCES" >> {log.out}        
        echo "Input AA file: {input.AA}" >> {log.out}
            
        # Run MitoGeneExtractor
        echo "===== Starting MitoGeneExtractor =====" >> {log.out}
        echo "Command: {params.mge_executor} -q {input.DNA} -p {input.AA} -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ -V {params.vulgar_dir} -r {wildcards.r} -s {wildcards.s} -n {params.n} -C {params.C} -t {params.t} --temporaryDirectory {params.output_dir} --verbosity 10" >> {log.out}
        echo "=======================================" >> {log.out}
        
        time {params.mge_executor} \\
        -q {input.DNA} -p {input.AA} \\
        -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \\
        -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \\
        -V {params.vulgar_dir} \\
        -r {wildcards.r} -s {wildcards.s} \\
        -n {params.n} \\
        -C {params.C} \\
        -t {params.t} \\
        --temporaryDirectory {params.output_dir} \\
        --verbosity 10 \\
        >> {log.out} 2>> {log.err} || {{
            EXIT_CODE=$?
            echo "===== MitoGeneExtractor Failed =====" >> {log.out}
            echo "Exit code: $EXIT_CODE" >> {log.out}
            echo "Finished: $(date)" >> {log.out}
            
            # Enhanced failure analysis
            if [ $EXIT_CODE -eq 137 ]; then
                echo "ANALYSIS: Process killed (likely out of memory)" >> {log.out}
                echo "  Current allocation: $(({resources.mem_mb} / 1024))GB" >> {log.out}
                echo "  File size: ${{FILE_SIZE_GB}}GB" >> {log.out}
                echo "  Sequences processed: $NUM_SEQUENCES" >> {log.out}
            elif [ $EXIT_CODE -eq 124 ]; then
                echo "ANALYSIS: Process timed out" >> {log.out}
            elif [ $EXIT_CODE -eq 1 ]; then
                echo "ANALYSIS: General error - check error log for details" >> {log.out}
            else
                echo "ANALYSIS: Unknown failure (exit code $EXIT_CODE)" >> {log.out}
            fi
            
            echo "Final system state:" >> {log.out}
            free -h >> {log.out}
            echo "===================================" >> {log.out}
            
            exit $EXIT_CODE
        }}

        echo "===== Job Completed Successfully =====" >> {log.out}
        echo "Finished: $(date)" >> {log.out}
        echo "Final memory usage:" >> {log.out}
        free -h >> {log.out}
        
        # Validate and report output files
        if [ -f "{output.consensus}" ] && [ -f "{output.alignment}" ]; then
            CONSENSUS_SIZE=$(stat -c%s {output.consensus})
            ALIGNMENT_SIZE=$(stat -c%s {output.alignment})
            CONSENSUS_LINES=$(wc -l < {output.consensus})
            
            echo "===== Output Summary =====" >> {log.out}
            echo "Consensus file: $(numfmt --to=iec $CONSENSUS_SIZE) ($CONSENSUS_LINES lines)" >> {log.out}
            echo "Alignment file: $(numfmt --to=iec $ALIGNMENT_SIZE)" >> {log.out}
            
            if [ $CONSENSUS_LINES -gt 1 ]; then
                echo "✓ Consensus contains sequence data" >> {log.out}
            else
                echo "⚠ WARNING: Consensus may be header-only" >> {log.out}
            fi
            echo "===========================" >> {log.out}
        else
            echo "ERROR: Expected output files not found" >> {log.out}
            echo "Expected files:" >> {log.out}
            echo "  Consensus: {output.consensus}" >> {log.out}
            echo "  Alignment: {output.alignment}" >> {log.out}
            exit 1
        fi
        """



# Rename headers in consensus files for merge mode
rule rename_and_combine_cons_merge:
    input:
        consensus_files=get_all_mge_consensus_files("merge")
    output:
        complete=os.path.join(output_dir_merge, "logs/rename_consensus/rename_complete.txt"),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_cons_combined-merge.fasta")
    log:
        os.path.join(output_dir_merge, "logs/rename_consensus/rename_fasta.log")
    params:
        script=os.path.join(workflow.basedir, "scripts/rename_headers.py"),
        preprocessing_mode="merge"
    threads: rule_resources["rename_and_combine_cons_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["rename_and_combine_cons_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """ 
        set -euo pipefail

		# Check all input files exist
        for file in {input.consensus_files}; do
            if [ ! -f "$file" ]; then
                echo "Error: Required input file does not exist: $file" >> {log}
                echo "Cannot proceed with renaming and combining consensus files." >> {log}
				echo "If any MitoGeneExtractor jobs failed, please rerun the workflow to generate those files." >> {log}

                exit 1
            fi
        done
		
        # Run script
        python {params.script} \\
            --input-files {input.consensus_files} \\
            --concatenated-consensus {output.concat_cons} \\
            --complete-file={output.complete} \\
            --log-file={log} \\
            --preprocessing-mode={params.preprocessing_mode} \\
            --threads={threads}
    
        # Touch completion file to ensure timestamp is updated
        touch {output.complete}
        """

# Rename headers in consensus files for concat mode
rule rename_and_combine_cons_concat:
    input:
        consensus_files=get_all_mge_consensus_files("concat")
    output:
        complete=os.path.join(output_dir_concat, "logs/rename_consensus/rename_complete.txt"),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_cons_combined-concat.fasta")
    log:
        os.path.join(output_dir_concat, "logs/rename_consensus/rename_fasta.log")
    params:
        script=os.path.join(workflow.basedir, "scripts/rename_headers.py"),
        preprocessing_mode="concat"
    threads: rule_resources["rename_and_combine_cons_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["rename_and_combine_cons_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """ 
        set -euo pipefail

		# Check all input files exist
        for file in {input.consensus_files}; do
            if [ ! -f "$file" ]; then
                echo "Error: Required input file does not exist: $file" >> {log}
                echo "Cannot proceed with renaming and combining consensus files." >> {log}
				echo "If any MitoGeneExtractor jobs failed, please rerun the workflow to generate those files." >> {log}
                exit 1
            fi
        done
		
        # Run script
        python {params.script} \\
            --input-files {input.consensus_files} \\
            --concatenated-consensus {output.concat_cons} \\
            --complete-file={output.complete} \\
            --log-file={log} \\
            --preprocessing-mode={params.preprocessing_mode} \\
            --threads={threads}
    
        # Touch completion file to ensure timestamp is updated
        touch {output.complete}
        """



# Remove exonerate intermediate files after consensus generation
rule remove_exonerate_intermediates:
    input:
        merge_consensus=os.path.join(output_dir_merge, f"consensus/{run_name}_cons_combined-merge.fasta"),
        concat_consensus=os.path.join(output_dir_concat, f"consensus/{run_name}_cons_combined-concat.fasta"),
        merge_rename_complete=os.path.join(output_dir_merge, "logs/rename_consensus/rename_complete.txt"),
        concat_rename_complete=os.path.join(output_dir_concat, "logs/rename_consensus/rename_complete.txt")
    output:
        merge_completion=os.path.join(output_dir_merge, "logs/exonerate_int_cleanup_complete.txt"),
        concat_completion=os.path.join(output_dir_concat, "logs/exonerate_int_cleanup_complete.txt")
    threads: rule_resources["remove_exonerate_intermediates"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["remove_exonerate_intermediates"]["mem_mb"] * attempt
    retries: 2
    run:     
        # Clean each mode directory and write separate completion files
        for mode_dir, mode_name, completion_file in [
            (output_dir_merge, "merge", output.merge_completion),
            (output_dir_concat, "concat", output.concat_completion)
        ]:
            removed_files = 0
            total_size_saved = 0
            
            with open(completion_file, 'w') as log:
                log.write(f"Starting exonerate intermediate file cleanup for {mode_name} mode: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                log.write(f"Mode directory: {mode_dir}\n\n")
                
                exonerate_pattern = os.path.join(mode_dir, "Concatenated_exonerate_input_*")
                exonerate_files = glob.glob(exonerate_pattern)
                
                if not exonerate_files:
                    log.write(f"No Concatenated_exonerate_input_* files found\n")
                else:
                    log.write(f"Found {len(exonerate_files)} files to remove:\n")
                    
                    for file_path in exonerate_files:
                        try:
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            
                            size_mb = file_size / (1024 * 1024)
                            log.write(f"  - Removed: {os.path.basename(file_path)} ({size_mb:.2f} MB)\n")
                            
                            removed_files += 1
                            total_size_saved += file_size
                            
                        except OSError as e:
                            log.write(f"  - ERROR removing {file_path}: {e}\n")
                
                # Summary
                total_saved_mb = total_size_saved / (1024 * 1024)
                log.write(f"\nCLEANUP SUMMARY for {mode_name} mode:\n")
                log.write(f"  Total files removed: {removed_files}\n")
                log.write(f"  Total disk space saved: {total_saved_mb:.2f} MB\n")
                log.write(f"  Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
			
			

# Create list of alignment files for stats for merge mode
rule create_alignment_log_merge:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir_merge, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_cons_combined-merge.fasta")
    output:
        alignment_log=os.path.join(output_dir_merge, "logs/mge/alignment_files.log")
    threads: rule_resources["create_alignment_log_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["create_alignment_log_merge"]["mem_mb"] * attempt
    retries: 3
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")

# Create list of alignment files for stats for concat mode
rule create_alignment_log_concat:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir_concat, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_cons_combined-concat.fasta")
    output:
        alignment_log=os.path.join(output_dir_concat, "logs/mge/alignment_files.log")
    threads: rule_resources["create_alignment_log_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["create_alignment_log_concat"]["mem_mb"] * attempt
    retries: 3
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")










# Sequential filtering rules (fasta_cleaner)
rule human_cox1_filter_merge:
    input:
        alignment_log=os.path.join(output_dir_merge, "logs/mge/alignment_files.log")
    output:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        metrics=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/01_human_cox1_filter.py"),
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered"),
        human_threshold=fasta_cleaner_params["human_threshold"]
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner/01_human_filter.log")
    threads: rule_resources["human_cox1_filter_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["human_cox1_filter_merge"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-log {input.alignment_log} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --metrics-csv {output.metrics} \\
            --human-threshold {params.human_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """

rule at_content_filter_merge:
    input:
        human_filtered=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        summary_metrics=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/02_at_content_filter.py"),
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered"),
        at_threshold=fasta_cleaner_params["at_difference"],
        at_mode=fasta_cleaner_params["at_mode"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner/02_at_filter.log")
    threads: rule_resources["at_content_filter_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["at_content_filter_merge"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python -u {params.script} \\
            --input_files {input.human_filtered} \\
            --output_dir {params.output_dir} \\
	        --filtered-files-list {output.filtered_files} \\
            --at_threshold {params.at_threshold} \\
            --at_mode {params.at_mode} \\
            --consensus_threshold {params.consensus_threshold} \\
            --threads {threads} \\
	        2>&1 | tee {log}
        """

rule statistical_outlier_filter_merge:
    input:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        summary_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        individual_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/03_statistical_outlier_filter.py"),
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered"),
        outlier_percentile=fasta_cleaner_params["outlier_percentile"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner/03_outlier_filter.log")
    threads: rule_resources["statistical_outlier_filter_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["statistical_outlier_filter_merge"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-files-list {input.filtered_files} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --summary-csv {output.individual_metrics} \\
            --metrics-csv {output.summary_metrics} \\
            --outlier-percentile {params.outlier_percentile} \\
            --consensus-threshold {params.consensus_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """

if use_reference_filtering:
    # Version WITH reference filtering - MERGE MODE
    rule reference_filter_merge:
        input:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filtered.txt"),
            metrics=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/04_reference_filter.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered"),
            reference_dir=reference_dir,
            filter_mode=fasta_cleaner_params["reference_filter_mode"],
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/04_reference_filter.log")
        threads: rule_resources["reference_filter_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["reference_filter_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            mkdir -p {params.output_dir}

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --filtered-files-list {output.filtered_files} \\
                --metrics-csv {output.metrics} \\
                --reference-dir {params.reference_dir} \\
                --filter-mode {params.filter_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_merge:
        input:
            human_metrics=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            reference_metrics=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-merge.csv")
        output:
            completion_flag=os.path.join(output_dir_merge, "logs/fasta_cleaner_complete.txt"),
            combined_statistics=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/06_aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner")
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --reference-metrics {input.reference_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """

    rule consensus_generation_merge:
        input:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/04_reference_filtered/reference_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_merge, "fasta_cleaner/cleaned_cons_combined.fasta"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-merge.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/05_consensus_generator.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="merge"
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """
else:
    # Version WITHOUT reference filtering
    rule consensus_generation_merge:
        input:
            filtered_files=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_merge, "fasta_cleaner/cleaned_cons_combined.fasta"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-merge.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/05_consensus_generator.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="merge"
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_merge:
        input:
            human_metrics=os.path.join(output_dir_merge, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_merge, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_merge, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_merge, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-merge.csv")
        output:
            completion_flag=os.path.join(output_dir_merge, "logs/fasta_cleaner_complete.txt"),
            combined_statistics=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/06_aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_merge, "fasta_cleaner")
        log:
            os.path.join(output_dir_merge, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_merge"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_merge"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """


# Sequential filtering rules (fasta_cleaner)
rule human_cox1_filter_concat:
    input:
        alignment_log=os.path.join(output_dir_concat, "logs/mge/alignment_files.log")
    output:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filtered.txt"),
        metrics=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/01_human_cox1_filter.py"),
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered"),
        human_threshold=fasta_cleaner_params["human_threshold"]
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner/01_human_filter.log")
    threads: rule_resources["human_cox1_filter_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["human_cox1_filter_concat"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-log {input.alignment_log} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --metrics-csv {output.metrics} \\
            --human-threshold {params.human_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """

rule at_content_filter_concat:
    input:
        human_filtered=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filtered.txt"),
        summary_metrics=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/02_at_content_filter.py"),
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered"),
        at_threshold=fasta_cleaner_params["at_difference"],
        at_mode=fasta_cleaner_params["at_mode"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner/02_at_filter.log")
    threads: rule_resources["at_content_filter_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["at_content_filter_concat"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python -u {params.script} \\
            --input_files {input.human_filtered} \\
            --output_dir {params.output_dir} \\
	        --filtered-files-list {output.filtered_files} \\
            --at_threshold {params.at_threshold} \\
            --at_mode {params.at_mode} \\
            --consensus_threshold {params.consensus_threshold} \\
            --threads {threads} \\
	        2>&1 | tee {log}
        """

rule statistical_outlier_filter_concat:
    input:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filtered.txt")
    output:
        filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt"),
        summary_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_summary_metrics.csv"),
        individual_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/03_statistical_outlier_filter.py"),
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered"),
        outlier_percentile=fasta_cleaner_params["outlier_percentile"],
        consensus_threshold=fasta_cleaner_params["consensus_threshold"]
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner/03_outlier_filter.log")
    threads: rule_resources["statistical_outlier_filter_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["statistical_outlier_filter_concat"]["mem_mb"] * attempt
    retries: 2
    shell:
        """
        set -euo pipefail

        mkdir -p {params.output_dir}
        
        python {params.script} \\
            --input-files-list {input.filtered_files} \\
            --output-dir {params.output_dir} \\
            --filtered-files-list {output.filtered_files} \\
            --summary-csv {output.individual_metrics} \\
            --metrics-csv {output.summary_metrics} \\
            --outlier-percentile {params.outlier_percentile} \\
            --consensus-threshold {params.consensus_threshold} \\
            --threads {threads} \\
            2>&1 | tee {log}
        """
		
if use_reference_filtering:
    # Version WITH reference filtering - CONCAT MODE  
    rule reference_filter_concat:
        input:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filtered.txt"),
            metrics=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/04_reference_filter.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered"),
            reference_dir=reference_dir,
            filter_mode=fasta_cleaner_params["reference_filter_mode"],
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/04_reference_filter.log")
        threads: rule_resources["reference_filter_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["reference_filter_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail
            
            mkdir -p {params.output_dir}
            
            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --filtered-files-list {output.filtered_files} \\
                --metrics-csv {output.metrics} \\
                --reference-dir {params.reference_dir} \\
                --filter-mode {params.filter_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule consensus_generation_concat:
        input:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/cleaned_cons_combined.fasta"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-concat.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/05_consensus_generator.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="concat"
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_concat:
        input:
            human_metrics=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            reference_metrics=os.path.join(output_dir_concat, "fasta_cleaner/04_reference_filtered/reference_filter_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-concat.csv")
        output:
            completion_flag=os.path.join(output_dir_concat, "logs/fasta_cleaner_complete.txt"),
            combined_statistics=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/06_aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner")
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --reference-metrics {input.reference_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """

else:
    # Version WITHOUT reference filtering
    rule consensus_generation_concat:
        input:
            filtered_files=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filtered.txt")
        output:
            concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/cleaned_cons_combined.fasta"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-concat.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/05_consensus_generator.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus"),
            consensus_threshold=fasta_cleaner_params["consensus_threshold"],
            preprocessing_mode="concat"
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/05_consensus_generation.log")
        threads: rule_resources["consensus_generation_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["consensus_generation_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --input-files-list {input.filtered_files} \\
                --output-dir {params.output_dir} \\
                --consensus-fasta {output.concat_consensus} \\
                --consensus-metrics {output.consensus_metrics} \\
                --consensus-threshold {params.consensus_threshold} \\
                --preprocessing-mode {params.preprocessing_mode} \\
                --threads {threads} \\
                2>&1 | tee {log}
            """

    rule aggregate_filter_metrics_concat:
        input:
            human_metrics=os.path.join(output_dir_concat, "fasta_cleaner/01_human_filtered/human_filter_metrics.csv"),
            at_metrics=os.path.join(output_dir_concat, "fasta_cleaner/02_at_filtered/at_filter_summary.csv"),
            outlier_metrics=os.path.join(output_dir_concat, "fasta_cleaner/03_outlier_filtered/outlier_filter_individual_metrics.csv"),
            consensus_metrics=os.path.join(output_dir_concat, "fasta_cleaner/05_cleaned_consensus/cleaned_cons_metrics-concat.csv")
        output:
            completion_flag=os.path.join(output_dir_concat, "logs/fasta_cleaner_complete.txt"),
            combined_statistics=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv")
        params:
            script=os.path.join(workflow.basedir, "scripts/06_aggregate_filter_metrics.py"),
            output_dir=os.path.join(output_dir_concat, "fasta_cleaner")
        log:
            os.path.join(output_dir_concat, "logs/fasta_cleaner/06_aggregate_metrics.log")
        threads: rule_resources["aggregate_filter_metrics_concat"]["threads"]
        resources:
            mem_mb=lambda wildcards, attempt: rule_resources["aggregate_filter_metrics_concat"]["mem_mb"] * attempt
        retries: 2
        shell:
            """
            set -euo pipefail

            python {params.script} \\
                --human-metrics {input.human_metrics} \\
                --at-metrics {input.at_metrics} \\
                --outlier-metrics {input.outlier_metrics} \\
                --consensus-metrics {input.consensus_metrics} \\
                --output-dir {params.output_dir} \\
                --combined-statistics {output.combined_statistics} \\
                --threads {threads} \\
                2>&1 | tee {log}
                
            touch {output.completion_flag}
            """


# Clean up intermediate fasta files from filtering directories (remove intermediate FASTA files)
rule remove_fasta_cleaner_files:
    input:
        # Wait for filtering to complete
        merge_cleaning_complete=os.path.join(output_dir_merge, "logs/fasta_cleaner_complete.txt"),
        concat_cleaning_complete=os.path.join(output_dir_concat, "logs/fasta_cleaner_complete.txt"),
        # Ensure final consensus sequences are created before cleanup
        merge_consensus=os.path.join(output_dir_merge, "fasta_cleaner/cleaned_cons_combined.fasta"),
        concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/cleaned_cons_combined.fasta")
    output:
        merge_cleanup_complete=os.path.join(output_dir_merge, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        concat_cleanup_complete=os.path.join(output_dir_concat, "logs/fasta_cleaner/fasta_cleaner_complete.txt")
    params:
        merge_dir=output_dir_merge,
        concat_dir=output_dir_concat,
        use_ref_filtering=str(use_reference_filtering)
    threads: rule_resources["remove_fasta_cleaner_files"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["remove_fasta_cleaner_files"]["mem_mb"] * attempt
    retries: 2
    run:
        def cleanup_fasta_files(mode_dir, mode_name, completion_file):       
            removed_files = 0
            total_size_saved = 0
            
            with open(completion_file, 'w') as log:
                log.write(f"Starting intermediate fasta cleanup for {mode_name} mode: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                log.write(f"Mode directory: {mode_dir}\n")
                log.write(f"Reference filtering enabled: {params.use_ref_filtering}\n\n")
                
                # Define directories to clean
                filter_dirs = [
                    "01_human_filtered",
                    "02_at_filtered", 
                    "03_outlier_filtered"
                ]
                
                # Add reference filtering directory if enabled
                if params.use_ref_filtering == "True":
                    filter_dirs.append("04_reference_filtered")
                    log.write("Including 04_reference_filtered directory in cleanup\n")
                else:
                    log.write("Skipping 04_reference_filtered directory (not enabled)\n")
                
                log.write(f"Directories to clean: {', '.join(filter_dirs)}\n\n")
                
                # Clean each filtering directory
                for filter_dir in filter_dirs:
                    dir_path = os.path.join(mode_dir, "fasta_cleaner", filter_dir)
                    
                    if not os.path.exists(dir_path):
                        log.write(f"[{filter_dir}] Directory not found: {dir_path}\n")
                        continue
                    
                    log.write(f"[{filter_dir}] Cleaning directory: {dir_path}\n")
                    
                    # Find all .fasta and .fas files
                    fasta_patterns = [
                        os.path.join(dir_path, "*.fasta"),
                        os.path.join(dir_path, "*.fas"),
                        os.path.join(dir_path, "**", "*.fasta"),
                        os.path.join(dir_path, "**", "*.fas")
                    ]
                    
                    fasta_files = []
                    for pattern in fasta_patterns:
                        fasta_files.extend(glob.glob(pattern, recursive=True))
                    
                    # Remove duplicates
                    fasta_files = list(set(fasta_files))
                    
                    if not fasta_files:
                        log.write(f"[{filter_dir}] No .fasta or .fas files found\n")
                        continue
                    
                    log.write(f"[{filter_dir}] Found {len(fasta_files)} fasta files to remove:\n")
                    
                    dir_removed = 0
                    dir_size_saved = 0
                    
                    for fasta_file in fasta_files:
                        try:
                            # Get file size before removal
                            file_size = os.path.getsize(fasta_file)
                            
                            # Remove the file
                            os.remove(fasta_file)
                            
                            # Log the removal
                            size_mb = file_size / (1024 * 1024)
                            log.write(f"  - Removed: {os.path.basename(fasta_file)} ({size_mb:.2f} MB)\n")
                            
                            dir_removed += 1
                            dir_size_saved += file_size
                            
                        except OSError as e:
                            log.write(f"  - ERROR removing {fasta_file}: {e}\n")
                    
                    removed_files += dir_removed
                    total_size_saved += dir_size_saved
                    
                    size_saved_mb = dir_size_saved / (1024 * 1024)
                    log.write(f"[{filter_dir}] Removed {dir_removed} files, saved {size_saved_mb:.2f} MB\n\n")
                
                # Summary
                total_saved_mb = total_size_saved / (1024 * 1024)
                log.write(f"CLEANUP SUMMARY for {mode_name} mode:\n")
                log.write(f"  Total files removed: {removed_files}\n")
                log.write(f"  Total disk space saved: {total_saved_mb:.2f} MB\n")
                log.write(f"  Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                
                return removed_files, total_saved_mb
        
        # Clean up merge mode - write directly to completion file
        merge_removed, merge_saved = cleanup_fasta_files(params.merge_dir, "merge", output.merge_cleanup_complete)
        
        # Clean up concat mode - write directly to completion file
        concat_removed, concat_saved = cleanup_fasta_files(params.concat_dir, "concat", output.concat_cleanup_complete)


# Run Fasta_compare script on MGE and fasta_cleaner concatenated consensus multi-FASTAs
rule fasta_compare:
    input:
        fasta_concat=os.path.join(output_dir_concat, f"consensus/{run_name}_cons_combined-concat.fasta"),
        fasta_merge=os.path.join(output_dir_merge, f"consensus/{run_name}_cons_combined-merge.fasta"),
        clean_fasta_concat=os.path.join(output_dir_concat, "fasta_cleaner/cleaned_cons_combined.fasta"),
        clean_fasta_merge=os.path.join(output_dir_merge, "fasta_cleaner/cleaned_cons_combined.fasta"),
        stats_concat=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        stats_merge=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
    output:
        csv=os.path.join(main_output_dir, f"fasta_compare/{run_name}_fasta_compare.csv"),
        full_fasta=os.path.join(main_output_dir, f"fasta_compare/{run_name}_full_sequences.fasta"),
        barcode_fasta=os.path.join(main_output_dir, f"fasta_compare/{run_name}_barcode_sequences.fasta")
    params:
        script=os.path.join(workflow.basedir, "scripts/fasta_compare.py"),
        target_marker=fasta_compare_params["target"],
        rank=fasta_compare_params.get("rank", 3),
        relaxed=fasta_compare_params.get("relaxed", False),
        verbosity=" ".join([
            "--verbose" if fasta_compare_params["verbose"] else "",
            "--relaxed" if fasta_compare_params.get("relaxed", False) else ""
        ]).strip()
    log:
        os.path.join(main_output_dir, f"fasta_compare/fasta_compare_{run_name}.log")
    threads: rule_resources["fasta_compare"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["fasta_compare"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        python {params.script} \
            --output-csv {output.csv} \
            --output-fasta {output.full_fasta} \
            --output-barcode {output.barcode_fasta} \
            --input {input.fasta_concat} {input.fasta_merge} {input.clean_fasta_concat} {input.clean_fasta_merge} \
            --target {params.target_marker} \
            --rank {params.rank} \
            --log-file {log} \
            {params.verbosity}
        """


# Extract, aggregate and calculate stats from inputs for merge mode
rule extract_stats_to_csv_merge:
    input:
        alignment_log=os.path.join(output_dir_merge, "logs/mge/alignment_files.log"),
        out_files=expand(
            os.path.join(output_dir_merge, "out/{sample}_r_{r}_s_{s}_summary.out"),
            sample=list(samples.keys()),
            r=config["r"],
            s=config["s"]
        ),
        pre_fasta=os.path.join(output_dir_merge, f"consensus/{run_name}_cons_combined-merge.fasta"),
        cleaning_csv=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        cleaner_complete=os.path.join(output_dir_merge, "logs/fasta_cleaner_complete.txt"),
        post_fasta=os.path.join(output_dir_merge, "fasta_cleaner/cleaned_cons_combined.fasta"),
        ref_seqs=os.path.join(main_output_dir, "references/sequence_references.csv") if run_gene_fetch else []
    output:
        stats=os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/mge_stats.py"),
        out_dir=os.path.join(output_dir_merge, "out/"),
        log_dir=os.path.join(output_dir_merge, "logs/mge"),
        ref_seqs_arg=lambda wildcards, input: f"--ref_seqs {input.ref_seqs}" if run_gene_fetch else ""
    threads: rule_resources["extract_stats_to_csv_merge"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["extract_stats_to_csv_merge"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
        # Run script
        python {params.script} -a {input.alignment_log} -o {output.stats} -od {params.out_dir} -c {input.cleaning_csv} {params.ref_seqs_arg} --pre-fasta {input.pre_fasta} --post-fasta {input.post_fasta}
        # Relocate logs
        mv mge_stats.log {params.log_dir}/
        """

rule extract_stats_to_csv_concat:
    input:
        alignment_log=os.path.join(output_dir_concat, "logs/mge/alignment_files.log"),
        out_files=expand(
            os.path.join(output_dir_concat, "out/{sample}_r_{r}_s_{s}_summary.out"),
            sample=list(samples.keys()),
            r=config["r"],
            s=config["s"]
        ),
        pre_fasta=os.path.join(output_dir_concat, f"consensus/{run_name}_cons_combined-concat.fasta"),
        cleaning_csv=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        cleaner_complete=os.path.join(output_dir_concat, "logs/fasta_cleaner_complete.txt"),
        post_fasta=os.path.join(output_dir_concat, "fasta_cleaner/cleaned_cons_combined.fasta"),
        ref_seqs=os.path.join(main_output_dir, "references/sequence_references.csv") if run_gene_fetch else []
    output:
        stats=os.path.join(output_dir_concat, f"{run_name}_cons_combined-concat-stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/mge_stats.py"),
        out_dir=os.path.join(output_dir_concat, "out/"),
        log_dir=os.path.join(output_dir_concat, "logs/mge"),
        ref_seqs_arg=lambda wildcards, input: f"--ref_seqs {input.ref_seqs}" if run_gene_fetch else ""
    threads: rule_resources["extract_stats_to_csv_concat"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["extract_stats_to_csv_concat"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail
        # Run script
        python {params.script} -a {input.alignment_log} -o {output.stats} -od {params.out_dir} -c {input.cleaning_csv} {params.ref_seqs_arg} --pre-fasta {input.pre_fasta} --post-fasta {input.post_fasta}
        # Relocate logs
        mv mge_stats.log {params.log_dir}/
        """


# Concatenate merge and concat stats CSV files with column alignment
rule combine_stats_files:
    input:
        merge_stats=os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv"),
        concat_stats=os.path.join(output_dir_concat, f"{run_name}_cons_combined-concat-stats.csv")
    output:
        combined_stats=os.path.join(main_output_dir, f"{run_name}_combined_stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/csv_combiner_mge.py")
    threads: rule_resources["combine_stats_files"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["combine_stats_files"]["mem_mb"] * attempt
    retries: 3
    shell:
        """
        set -euo pipefail

        python {params.script} \
            --input {input.merge_stats} {input.concat_stats} \
            --output {output.combined_stats}
        """	


# Final clean up superfluous files
rule cleanup_files:
    input:
        # Merge mode inputs
        merge_summary_csv=os.path.join(output_dir_merge, f"{run_name}_merge-stats.csv"),
        clean_headers_log=os.path.join(output_dir_merge, "logs/clean_headers/clean_headers.log"),
        merge_exon_remove=os.path.join(output_dir_merge, "logs/exonerate_int_cleanup_complete.txt"),
        # Concat mode inputs
        concat_summary_csv=os.path.join(output_dir_concat, f"{run_name}_cons_combined-concat-stats.csv"),
        concat_logs=os.path.join(output_dir_concat, "logs/concat/concat_reads.log"),
        trim_galore_logs=os.path.join(output_dir_concat, "logs/trim_galore/trim_galore.log"),
        concat_exon_remove=os.path.join(output_dir_concat, "logs/exonerate_int_cleanup_complete.txt"),
        # Wait for fasta cleanup to complete
        merge_fasta_cleanup=os.path.join(output_dir_merge, "logs/fasta_cleaner/fasta_cleaner_complete.txt"),
        concat_fasta_cleanup=os.path.join(output_dir_concat, "logs/fasta_cleaner/fasta_cleaner_complete.txt")
    output:
        merge_complete=touch(os.path.join(output_dir_merge, "logs/final_cleanup_complete.txt")),
        concat_complete=touch(os.path.join(output_dir_concat, "logs/final_cleanup_complete.txt"))
    threads: rule_resources["cleanup_files"]["threads"]
    resources:
        mem_mb=lambda wildcards, attempt: rule_resources["cleanup_files"]["mem_mb"] * attempt
    retries: 2
    run:      
        # Function to clean up a specific mode's files
        def cleanup_mode_files(mode_dir, mode_name):
            removed_files = 0
            removed_dirs = []
            cleaned_dirs = []
            
            # Remove empty .log files in mge subdir - both modes
            mge_log_dir = os.path.join(mode_dir, "logs/mge")
            if os.path.exists(mge_log_dir):
                # Find all .log files in subdirectories of mge/
                mge_log_files = glob.glob(os.path.join(mge_log_dir, "**", "*.log"), recursive=True)
                removed_empty_logs = 0
                skipped_nonempty_logs = 0
                
                for mge_log_file in mge_log_files:
                    # Check if the log file is empty (0 bytes)
                    if os.path.getsize(mge_log_file) == 0:
                        print(f"[{mode_name}] Removing empty MGE log file: {mge_log_file}")
                        os.remove(mge_log_file)
                        removed_empty_logs += 1
                    else:
                        print(f"[{mode_name}] Keeping non-empty MGE log file: {mge_log_file} ({os.path.getsize(mge_log_file)} bytes)")
                        skipped_nonempty_logs += 1
                
                print(f"[{mode_name}] Removed {removed_empty_logs} empty MGE log files")
                print(f"[{mode_name}] Kept {skipped_nonempty_logs} non-empty MGE log files")
                removed_files += removed_empty_logs
                
                cleaned_dirs.append(mge_log_dir)
            else:
                print(f"[{mode_name}] MGE log directory not found: {mge_log_dir}")
            
            # Remove rename_complete.txt - both modes
            rename_complete_file = os.path.join(mode_dir, "logs/rename_consensus/rename_complete.txt")
            if os.path.exists(rename_complete_file):
                print(f"[{mode_name}] Removing rename_complete.txt file: {rename_complete_file}")
                os.remove(rename_complete_file)
                removed_files += 1
            else:
                print(f"[{mode_name}] rename_complete.txt file not found: {rename_complete_file}")
            
            # Return cleanup statistics
            return {
                "removed_files": removed_files,
                "removed_dirs": removed_dirs,
                "cleaned_dirs": cleaned_dirs
            }
        
        # Cleanup for merge mode
        merge_stats = cleanup_mode_files(output_dir_merge, "merge")
        
        # Cleanup for concat mode
        concat_stats = cleanup_mode_files(output_dir_concat, "concat")
        
        # Write completion message and cleanup info to merge output file
        with open(output.merge_complete, 'w') as f:
            f.write("Cleanup complete at " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            f.write(f"Total log files removed: {merge_stats['removed_files']}\n")            
            f.write("Directories fully removed:\n")
            if merge_stats['removed_dirs']:
                for dir_path in merge_stats['removed_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were fully removed\n")
            
            f.write("\nDirectories cleaned of log files:\n")
            if merge_stats['cleaned_dirs']:
                for dir_path in merge_stats['cleaned_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were cleaned\n")
            
            f.write("\nPreprocessing mode: merge")
        
        # Write completion message and cleanup info to concat output file
        with open(output.concat_complete, 'w') as f:
            f.write("Cleanup complete at " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            f.write(f"Total log files removed: {concat_stats['removed_files']}\n")
            
            f.write("Directories fully removed:\n")
            if concat_stats['removed_dirs']:
                for dir_path in concat_stats['removed_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were fully removed\n")
            
            f.write("\nDirectories cleaned of log files:\n")
            if concat_stats['cleaned_dirs']:
                for dir_path in concat_stats['cleaned_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were cleaned\n")
            
            f.write("\nPreprocessing mode: concat")
