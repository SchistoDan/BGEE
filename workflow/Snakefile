import os
import csv
import yaml
from snakemake.io import expand
import pandas as pd
import glob
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from threading import Lock


# ===== CONFIGURATION AND VALIDATION =====

# Config validation
def validate_config(config):
    required_keys = ['samples_file', 'sequence_reference_file', 'output_dir', 'r', 's', 'run_name', 'mge_path']
    missing_keys = [key for key in required_keys if key not in config]
    
    if missing_keys:
        sys.stderr.write(f"WARNING: Missing required configuration keys: {', '.join(missing_keys)}\n")
        sys.exit(1)
    
    # Validate file existence
    if not os.path.exists(config['samples_file']):
        sys.stderr.write(f"WARNING: Samples file not found: {config['samples_file']}\n")
        sys.exit(1)
    
    if not os.path.exists(config['sequence_reference_file']):
        sys.stderr.write(f"WARNING: Sequence reference file not found: {config['sequence_reference_file']}\n")
        sys.exit(1)
    
    if not os.path.exists(config['mge_path']):
        sys.stderr.write(f"WARNING: MitoGeneExtractor path not found: {config['mge_path']}\n")
        sys.exit(1)
    
    # Validate r and s parameters
    if not isinstance(config['r'], list) or not config['r']:
        sys.stderr.write("WARNING: 'r' must be a non-empty list\n")
        sys.exit(1)
    
    if not isinstance(config['s'], list) or not config['s']:
        sys.stderr.write("WARNING: 's' must be a non-empty list\n")
        sys.exit(1)

# Validate configuration file
validate_config(config)

# Print configuration for debugging
print("\n=====Configuration loaded:=====\n", config) 


     

 
# Load cluster config
try:
    with open("./config/cluster_config.yaml") as f:
        cluster_config = yaml.safe_load(f)
    print("\n=====Cluster configuration loaded successfully from ./config/cluster_config.yaml=====")
except Exception as e:
    print(f"WARNING: Failed to load cluster_config.yaml: {e}")
    print("WARNING: Using empty cluster configuration as fallback")
    cluster_config = {}

# Validate cluster configuration
def validate_cluster_config(cluster_config):
    """Validate the cluster configuration for required structure and sensible defaults"""
    # Check if __default__ section exists
    if "__default__" not in cluster_config:
        print("WARNING: No '__default__' section found in cluster_config.yaml. This is recommended for fallback values.")
    
    # Check each rule has sensible mem and CPU values
    for rule, settings in cluster_config.items():
        if "mem" not in settings:
            print(f"WARNING: No 'mem' setting for rule '{rule}' in cluster_config.yaml")
        if "cpus-per-task" not in settings:
            print(f"WARNING: No 'cpus-per-task' setting for rule '{rule}' in cluster_config.yaml")
            
    return True

# Validate the cluster config
validate_cluster_config(cluster_config)



# ===== RESOURCE MANAGEMENT FUNCTIONS =====

def get_mem_mb_from_config(rule_name, default=12288):
    """
    Get memory allocation from cluster config in MB for scaling with attempts.
    """
    # Check if rule exists in cluster config
    if rule_name in cluster_config:
        mem_str = cluster_config[rule_name].get("mem", str(default))
    # Otherwise use default section
    elif "__default__" in cluster_config:
        mem_str = cluster_config["__default__"].get("mem", str(default))
    else:
        mem_str = str(default)
    
    # Convert from string (e.g. "4G") to MB
    if isinstance(mem_str, str):
        if mem_str.endswith("G"):
            return int(float(mem_str.rstrip("G")) * 1024)
        elif mem_str.endswith("M"):
            return int(float(mem_str.rstrip("M")))
        else:
            try:
                return int(float(mem_str))
            except ValueError:
                return default
    else:
        return int(mem_str)

def get_cluster_threads(rule_name, default=2):
    """
	Get CPU count from cluster config for each rule.
	If rule not listed in cluster config then use default allocation.
	"""
    # Check if rule exists in cluster config
    if rule_name in cluster_config:
        return cluster_config[rule_name].get("cpus-per-task", default)
    # Otherwise use default section
    elif "__default__" in cluster_config:
        return cluster_config["__default__"].get("cpus-per-task", default)
    else:
        return default

# Print resource allocation for each rule
print("=====Rule resource allocations:")
for rule_name in cluster_config:
    if rule_name != "__default__":
        mem = cluster_config[rule_name].get("mem", "Using default")
        cpus = cluster_config[rule_name].get("cpus-per-task", "Using default")
        print(f"  - {rule_name}: mem={mem}, cpus-per-task={cpus}")




# ===== DATA PARSING FUNCTIONS =====

# Parse samples from .csv files
def parse_samples(samples_file):
    samples = {}
    with open(samples_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['ID']
            forward_read = row['forward']
            reverse_read = row['reverse']
            samples[sample_id] = {"R1": forward_read, "R2": reverse_read}
    return samples

# Parse references from CSV
def parse_sequence_references(sequence_reference_file):
    sequence_references = {}
    with open(sequence_reference_file, mode='r') as infile:
        reader = csv.DictReader(infile)
        for row in reader:
            sample_id = row['process_id']
            reference_name = row['reference_name']
            protein_reference_path = row['protein_reference_path']
            nucleotide_reference_path = row['nucleotide_reference_path']
            sequence_references[sample_id] = {
                "reference_name": reference_name, 
                "protein_path": protein_reference_path,
                "nucleotide_path": nucleotide_reference_path
            }    
    return sequence_references



# ===== PARAMETER SETUP =====

# Load parameters from config file
samples = parse_samples(config["samples_file"])
sequence_references = parse_sequence_references(config["sequence_reference_file"])


# Create main output directory
main_output_dir = config["output_dir"]
Path(main_output_dir).mkdir(parents=True, exist_ok=True)

# Create separate output directories for each preprocessing mode
output_dir_merge = os.path.join(main_output_dir, "merge_mode")
output_dir_concat = os.path.join(main_output_dir, "concat_mode")


# Required parameters
r = config["r"]
s = config["s"]
run_name = config["run_name"]
mge_path = config["mge_path"]


# Fasta-cleaner and Fasta_compare parameters with defaults
fasta_cleaner_params = config.get("fasta_cleaner", {
    "consensus_threshold": 0.5,
    "human_threshold": 0.95,
    "at_difference": 0.1,
    "at_mode": "absolute",
    "outlier_percentile": 90.0,
    "disable_human": False,
    "disable_at": False,
    "disable_outliers": False
})

fasta_compare_params = config.get("fasta_compare", {
    "target": "cox1",
    "verbose": False
})




# ===== SAMPLE AND REFERENCE PREPARATION =====

#Create lists of samples and corresponding references, and link them
sample_list = list(samples.keys())
reference_list = [sequence_references[sample]["reference_name"] for sample in sample_list]
sample_reference_pairs = list(zip(sample_list, reference_list))

# Generate all combinations for each sample+reference pair with all r and s values
def generate_mge_cons_paths(output_dir, sample_ref_pairs, r_values, s_values):
    paths = []
    for sample, ref in sample_ref_pairs:
        for r_val in r_values:
            for s_val in s_values:
                paths.append(
                    os.path.join(output_dir, f"consensus/{sample}_r_{r_val}_s_{s_val}_con_{ref}.fas")
                )
    return paths

all_mge_cons_merge = generate_mge_cons_paths(output_dir_merge, sample_reference_pairs, r, s)
all_mge_cons_concat = generate_mge_cons_paths(output_dir_concat, sample_reference_pairs, r, s)


# ===== DIRECTORY SETUP AND LOGGING =====

# Print run parameters from config.yaml
print("\n===Parameters loaded from config.yaml===")
print(f"-*-Run name: {run_name}\n")
print(f"-*-Output directories: {output_dir_merge} (merge mode), {output_dir_concat} (concat mode)\n")
print(f"-*-r params: {r}")
print(f"-*-s params: {s}\n")
print("-*-Sample files:", samples, "\n")
print("-*-Sequence references:", sequence_references, "\n")
print("-*-reference list:", reference_list, "\n")
print("-*-sample reference pairs:", sample_reference_pairs, "\n")







# ===== HELPER FUNCTIONS FOR MODE-SPECIFIC RULES =====

# Get the right input for merge mode
def get_mge_input_merge(wildcards):
    return os.path.join(output_dir_merge, f"trimmed_data/{wildcards.sample}_merged_clean.fq")

# Get input for concat mode
def get_mge_input_concat(wildcards):
    return os.path.join(output_dir_concat, f"trimmed_data/{wildcards.sample}_concat_trimmed.fq")




# ===== SNAKEMAKE RULES =====

# Rule them all
rule all:
    input:
        # ==== MERGE MODE OUTPUTS ====
        # Log files
        os.path.join(output_dir_merge, "logs/clean_headers.log"),
        
        # Consensus files
        all_mge_cons_merge,
        os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        
        # Additional outputs
        os.path.join(output_dir_merge, "logs/alignment_files.log"),
        os.path.join(output_dir_merge, "logs/rename_complete.txt"),
        os.path.join(output_dir_merge, f"{run_name}_merge.csv"),
        os.path.join(output_dir_merge, "logs/cleaning_complete.txt"),
        os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        os.path.join(output_dir_merge, "cleanup_complete.txt"),
        
        # ==== CONCAT MODE OUTPUTS ====
        # Log files
        os.path.join(output_dir_concat, "logs/concat_reads.log"),
        os.path.join(output_dir_concat, "logs/trim_galore.log"),
        
        # Consensus files  
        all_mge_cons_concat,
        os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        
        # Additional outputs
        os.path.join(output_dir_concat, "logs/alignment_files.log"),
        os.path.join(output_dir_concat, "logs/rename_complete.txt"),
        os.path.join(output_dir_concat, f"{run_name}_concat.csv"),
        os.path.join(output_dir_concat, "logs/cleaning_complete.txt"),
        os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        os.path.join(output_dir_concat, "cleanup_complete.txt"),
		
        # ==== FASTA COMPARE OUTPUTS ====
        os.path.join(main_output_dir, f"fasta_compare/{run_name}_fasta_compare.csv"),
        os.path.join(main_output_dir, f"fasta_compare/{run_name}_full_sequences.fasta"),
        os.path.join(main_output_dir, f"fasta_compare/{run_name}_barcode_sequences.fasta"), 
		
		# ==== FINAL STATS OUTPUT ====
        os.path.join(main_output_dir, f"{run_name}_combined_stats.csv")


# ===== MERGE MODE RULES =====
rule fastp_pe_merge:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=os.path.join(output_dir_merge, "trimmed_data/{sample}_R1_trimmed.fq.gz"),
        R2_trimmed=os.path.join(output_dir_merge, "trimmed_data/{sample}_R2_trimmed.fq.gz"),
        report=os.path.join(output_dir_merge, "trimmed_data/reports/{sample}_fastp_report.html"),
        json=os.path.join(output_dir_merge, "trimmed_data/reports/{sample}_fastp_report.json"),
        merged_reads=temp(os.path.join(output_dir_merge, "trimmed_data/{sample}_merged.fq")),
        unpaired_R1=temp(os.path.join(output_dir_merge, "trimmed_data/unpaired/{sample}_unpaired_R1.fq")),
        unpaired_R2=temp(os.path.join(output_dir_merge, "trimmed_data/unpaired/{sample}_unpaired_R2.fq"))
    log:
        out=os.path.join(output_dir_merge, "trimmed_data/{sample}_fastp.out"),
        err=os.path.join(output_dir_merge, "trimmed_data/{sample}_fastp.err")
    threads: lambda wildcards: get_cluster_threads("fastp_pe_merge")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("fastp_pe_merge")
    retries: 3
    shell:
        """
        # Log job info
        echo "Starting fastp merge for sample {wildcards.sample}" > {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Input files: {input.R1}, {input.R2}" >> {log.out}

        fastp -i {input.R1} -I {input.R2} \
              -o {output.R1_trimmed} -O {output.R2_trimmed} \
              --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \
              --adapter_sequence_r2=AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
              --dedup \
              --trim_poly_g \
              --merge --merged_out {output.merged_reads} \
              --unpaired1 {output.unpaired_R1} \
              --unpaired2 {output.unpaired_R2} \
              -h {output.report} -j {output.json} \
              > {log.out} 2> {log.err}

        # Log job completion and disk usage
        echo "Completed: $(date)" >> {log.out}
        echo "Output files sizes:" >> {log.out}
        du -h {output.R1_trimmed} {output.R2_trimmed} {output.merged_reads} >> {log.out}
        """

rule clean_headers_merge:
    input:
        merged_reads=os.path.join(output_dir_merge, "trimmed_data/{sample}_merged.fq")
    output:
        clean_merged=os.path.join(output_dir_merge, "trimmed_data/{sample}_merged_clean.fq")
    log:            
        os.path.join(output_dir_merge, "logs/clean_headers/{sample}.log")
    threads: lambda wildcards: get_cluster_threads("clean_headers_merge")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("clean_headers_merge")
    retries: 3
    shell:
        """
        # Create directories first to avoid race conditions
        mkdir -p $(dirname {log})
        mkdir -p $(dirname {output.clean_merged})
        
        # Create log header with timestamp and sample ID
        echo "Cleaning headers for {wildcards.sample}" > {log}
        echo "Started: $(date)" >> {log}
        echo "Host: $(hostname)" >> {log}
        
        # Add robust error handling with retries
        for attempt in $(seq 1 3); do
            echo "Attempt $attempt of 3" >> {log}
            
            if ! [ -r "{input.merged_reads}" ]; then
                echo "Input file not readable, waiting 10 seconds..." >> {log}
                sleep 10
                continue
            fi
            
            # Use perl with temporary file approach to be more robust
            TMP_OUTPUT=$(mktemp)
            if perl -pe 's/ /_/g' {input.merged_reads} > $TMP_OUTPUT; then
                # Move the temp file to the final location only if successful
                mv $TMP_OUTPUT {output.clean_merged}
                echo "Completed: $(date)" >> {log}
                echo "Output file size: $(du -h {output.clean_merged} | cut -f1)" >> {log}
                echo "--------------------------------------------" >> {log}
                exit 0
            else
                echo "Perl command failed on attempt $attempt" >> {log}
                rm -f $TMP_OUTPUT
                sleep 5
            fi
        done
        
        # If all attempts fail
        echo "FAILED after 3 attempts: $(date)" >> {log}
        exit 1
        """

rule aggregate_clean_headers_logs_merge:
    input:
        sample_logs=expand(os.path.join(output_dir_merge, "logs/clean_headers/{sample}.log"), sample=sample_list)
    output:
        combined_log=os.path.join(output_dir_merge, "logs/clean_headers.log")
    threads: lambda wildcards: get_cluster_threads("aggregate_clean_headers_logs")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("aggregate_clean_headers_logs")
    shell:
        """
        # Create header for aggregated log file
        echo "===============================================" > {output.combined_log}
        echo "Aggregated clean headers logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}
        
        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log}
        """

# Helper function to get the right input for merge mode
def get_mge_input_merge(wildcards):
    return os.path.join(output_dir_merge, f"trimmed_data/{wildcards.sample}_merged_clean.fq")


# ===== CONCAT MODE RULES =====

rule fastp_pe_concat:
    input:
        R1=lambda wildcards: samples[wildcards.sample]["R1"],
        R2=lambda wildcards: samples[wildcards.sample]["R2"]
    output:
        R1_trimmed=os.path.join(output_dir_concat, "trimmed_data/{sample}_R1_trimmed.fastq"),
        R2_trimmed=os.path.join(output_dir_concat, "trimmed_data/{sample}_R2_trimmed.fastq"),
        report=os.path.join(output_dir_concat, "trimmed_data/reports/{sample}_fastp_report.html"),
        json=os.path.join(output_dir_concat, "trimmed_data/reports/{sample}_fastp_report.json")
    log:
        out=os.path.join(output_dir_concat, "trimmed_data/{sample}_fastp.out"),
        err=os.path.join(output_dir_concat, "trimmed_data/{sample}_fastp.err")
    threads: lambda wildcards: get_cluster_threads("fastp_pe_concat")  
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("fastp_pe_concat")  
    retries: 3
    shell:
        """
        # Create output directories if they don't exist
        mkdir -p $(dirname {output.R1_trimmed})
        mkdir -p $(dirname {output.report})
        
        # Create a consistent log header with timestamp and sample ID
        LOG_HEADER="[$(date '+%Y-%m-%d %H:%M:%S')] Sample: {wildcards.sample}"
        
        # Initialize the log files
        echo "$LOG_HEADER - Starting fastp processing (attempt ${{SLURM_RESTART_COUNT:-1}})" > {log.out}
        echo "$LOG_HEADER - Input files: {input.R1}, {input.R2}" >> {log.out}
        echo "$LOG_HEADER - Output files: {output.R1_trimmed}, {output.R2_trimmed}" >> {log.out}
        
        # Log input file sizes and existence
        echo "$LOG_HEADER - Input file details:" >> {log.out}
        if [[ -f "{input.R1}" ]]; then
            echo "  R1: $(du -h {input.R1} | cut -f1) ($(stat -c%s {input.R1}) bytes)" >> {log.out}
        else
            echo "  ERROR: R1 input file does not exist" >> {log.out}
            exit 1
        fi
        
        if [[ -f "{input.R2}" ]]; then
            echo "  R2: $(du -h {input.R2} | cut -f1) ($(stat -c%s {input.R2}) bytes)" >> {log.out}
        else
            echo "  ERROR: R2 input file does not exist" >> {log.out}
            exit 1
        fi
        
        # Run fastp with comprehensive logging
        echo "$LOG_HEADER - Starting fastp: $(date)" >> {log.out}
        
        # Execute fastp with error handling
        if fastp -i {input.R1} -I {input.R2} \\
                 -o {output.R1_trimmed} -O {output.R2_trimmed} \\
                 -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA \\
                 -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \\
                 --dedup \\
                 --trim_poly_g \\
                 --thread {threads} \\
                 -h {output.report} -j {output.json} \\
                 >> {log.out} 2> {log.err}; then
            echo "$LOG_HEADER - Fastp completed successfully: $(date)" >> {log.out}
        else
            FASTP_EXIT=$?
            echo "$LOG_HEADER - ERROR: Fastp failed with exit code $FASTP_EXIT: $(date)" >> {log.out}
            
            # Examine error log for specific issues
            if grep -q "different read numbers" {log.err}; then
                echo "$LOG_HEADER - ERROR: Read count mismatch detected in input files" >> {log.out}
                echo "$LOG_HEADER - Error details from fastp:" >> {log.out}
                grep -A 5 "different read numbers" {log.err} >> {log.out}
            fi
            
            # Check if output files were at least partially created
            if [[ -f {output.R1_trimmed} ]]; then
                echo "$LOG_HEADER - R1 output was created but may be incomplete: $(du -h {output.R1_trimmed} | cut -f1)" >> {log.out}
            fi
            
            if [[ -f {output.R2_trimmed} ]]; then
                echo "$LOG_HEADER - R2 output was created but may be incomplete: $(du -h {output.R2_trimmed} | cut -f1)" >> {log.out}
            fi
            
            # Propagate the error to trigger a retry
            exit $FASTP_EXIT
        fi
        
        # Validate output files
        if [[ ! -s {output.R1_trimmed} ]]; then
            echo "$LOG_HEADER - ERROR: R1 output file is empty after processing" >> {log.out}
            exit 1
        fi
        
        if [[ ! -s {output.R2_trimmed} ]]; then
            echo "$LOG_HEADER - ERROR: R2 output file is empty after processing" >> {log.out}
            exit 1
        fi
        
        # Calculate and log read counts from output files
        echo "$LOG_HEADER - Calculating read counts in output files..." >> {log.out}
        R1_READS=$(($(wc -l < {output.R1_trimmed}) / 4))
        R2_READS=$(($(wc -l < {output.R2_trimmed}) / 4))
        echo "$LOG_HEADER - Read counts: R1=$R1_READS reads, R2=$R2_READS reads" >> {log.out}
        
        # Verify read counts match between R1 and R2
        if [[ $R1_READS -ne $R2_READS ]]; then
            echo "$LOG_HEADER - WARNING: Read count mismatch in output files (R1=$R1_READS, R2=$R2_READS)" >> {log.out}
            # Not failing here since the files were created, but logging the issue
        fi
        
        # Clean headers
        echo "$LOG_HEADER - Cleaning headers in output files" >> {log.out}
        perl -i -pe 's/ /_/g' {output.R1_trimmed}
        perl -i -pe 's/ /_/g' {output.R2_trimmed}
        echo "$LOG_HEADER - Headers cleaned" >> {log.out}
        
        # Log detailed output file information
        echo "$LOG_HEADER - Output file details:" >> {log.out}
        echo "  R1: $(du -h {output.R1_trimmed} | cut -f1) ($(stat -c%s {output.R1_trimmed}) bytes)" >> {log.out}
        echo "  R2: $(du -h {output.R2_trimmed} | cut -f1) ($(stat -c%s {output.R2_trimmed}) bytes)" >> {log.out}
        
        # Log successful completion
        echo "$LOG_HEADER - Processing completed successfully at $(date)" >> {log.out}
        echo "$LOG_HEADER - ----------------------------------------" >> {log.out}
        """


rule fastq_concat:
    input:
        R1=os.path.join(output_dir_concat, "trimmed_data/{sample}_R1_trimmed.fastq"),
        R2=os.path.join(output_dir_concat, "trimmed_data/{sample}_R2_trimmed.fastq")
    output:
        temp(os.path.join(output_dir_concat, "trimmed_data/{sample}_concat.fastq"))
    log:
        os.path.join(output_dir_concat, "logs/concat/{sample}.log")
    threads: lambda wildcards: get_cluster_threads("fastq_concat")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("fastq_concat")
    retries: 3
    shell:
        """
        echo "Concatenating files for {wildcards.sample}" > {log}
        echo "Started: $(date)" >> {log}
        
        # Concatenation
        (cat {input.R1} && cat {input.R2}) > {output}
        
        echo "Completed: $(date)" >> {log}
        echo "Output file size: $(du -h {output} | cut -f1)" >> {log}
        """

rule aggregate_concat_logs:
    input:
        sample_logs=expand(os.path.join(output_dir_concat, "logs/concat/{sample}.log"), sample=sample_list)
    output:
        combined_log=os.path.join(output_dir_concat, "logs/concat_reads.log")
    threads: lambda wildcards: get_cluster_threads("aggregate_concat_logs")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("aggregate_concat_logs")
    shell:
        """
        echo "===============================================" > {output.combined_log}
        echo "Aggregated concat logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}
    
        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log}
        """

rule quality_trim:
    input:
        os.path.join(output_dir_concat, "trimmed_data/{sample}_concat.fastq")
    output:
        temp(os.path.join(output_dir_concat, "trimmed_data/{sample}_concat_trimmed.fq")),
        report=os.path.join(output_dir_concat, "trimmed_data/reports/{sample}_concat.fastq_trimming_report.txt")
    params:
        outdir=os.path.join(output_dir_concat, "trimmed_data/"),
        report_dir=os.path.join(output_dir_concat, "trimmed_data/reports/")
    log:
        os.path.join(output_dir_concat, "logs/trim_galore/{sample}.log")
    threads: lambda wildcards: get_cluster_threads("quality_trim")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("quality_trim")
    retries: 3
    shell:
        """
        echo "Quality trimming for {wildcards.sample}" > {log}
        echo "Started: $(date)" >> {log}
        echo "Input file: {input}" >> {log}
		
		# Run trim_galore
        trim_galore --cores {threads} \
                   --dont_gzip \
                   --output_dir {params.outdir} \
                   --basename {wildcards.sample}_concat \
                   {input}

        # Move reports to out location
        mv {params.outdir}/{wildcards.sample}_concat.fastq_trimming_report.txt {output.report}

        # Log job completion
        echo "Completed: $(date)" >> {log}
        echo "Output file size: $(du -h {output[0]} | cut -f1)" >> {log}
        """

rule aggregate_trim_galore_logs:
    input:
        sample_logs=expand(os.path.join(output_dir_concat, "logs/trim_galore/{sample}.log"), sample=sample_list)
    output:
        combined_log=os.path.join(output_dir_concat, "logs/trim_galore.log")
    threads: lambda wildcards: get_cluster_threads("aggregate_trim_galore_logs")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("aggregate_trim_galore_logs")
    shell:
        """
        echo "===============================================" > {output.combined_log}
        echo "Aggregated trim_galore logs - Created $(date)" >> {output.combined_log}
        echo "===============================================" >> {output.combined_log}
        echo "" >> {output.combined_log}
    
        # Concatenate all sample logs into a combined log
        cat {input.sample_logs} >> {output.combined_log}
        """

# Helper function to get the right input for concat mode
def get_mge_input_concat(wildcards):
    return os.path.join(output_dir_concat, f"trimmed_data/{wildcards.sample}_concat_trimmed.fq")





# ===== SHARED RULES WITH MODE-SPECIFIC VERSIONS =====

# MGE rule for merge mode
rule MitoGeneExtractor_merge:
    input:
        DNA=get_mge_input_merge,
        AA=lambda wildcards: sequence_references[wildcards.sample]["protein_path"]
    output:
        alignment=os.path.join(output_dir_merge, "alignment/{sample}_r_{r}_s_{s}_align_{reference_name}.fas"),
        consensus=os.path.join(output_dir_merge, "consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas"),
        vulgar=os.path.join(output_dir_merge, "logs/mge/{sample}_vulgar_r_{r}_s_{s}_{reference_name}.txt")
    log:
        out=os.path.join(output_dir_merge, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
        err=os.path.join(output_dir_merge, "err/{sample}_r_{r}_s_{s}_summary_{reference_name}.err")
    params:
        mge_executor=config["mge_path"],
        output_dir=output_dir_merge
    threads: lambda wildcards: get_cluster_threads("MitoGeneExtractor_merge")
    resources:
        mem_mb=lambda wildcards, attempt: (get_mem_mb_from_config("MitoGeneExtractor_merge")) * attempt
    retries: 3
    shell:
        """
        # Clear previous log file for clean output on retries
        > {log.out}
        
        # Logging header with improved attempt tracking
        echo "===== Job Info =====" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Node: $(hostname)" >> {log.out}
        echo "Memory allocated: {resources.mem_mb}MB" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "Mode: concat" >> {log.out}
        echo "===================" >> {log.out}
        		
        # Log input files sizes
        echo "Input DNA file size: $(du -h {input.DNA} | cut -f1)" >> {log.out}
        echo "Input AA file: {input.AA}" >> {log.out}
        echo "Available system memory before execution:" >> {log.out}
        free -h >> {log.out}
             
        # Run MGE & track resource usage
        /usr/bin/time -v {params.mge_executor} \\
        -q {input.DNA} -p {input.AA} \\
        -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \\
        -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \\
        -V {output.vulgar} \\
        -r {wildcards.r} -s {wildcards.s} \\
        -n 0 -C 5 -t 0.5 \\
        --verbosity 4 \\
        >> {log.out} 2>> {log.err} || {{
            # If the command fails, log error and system state
            EXIT_CODE=$?
            echo "MitoGeneExtractor failed with exit code $EXIT_CODE" >> {log.out}
            echo "Available system memory after failure:" >> {log.out}
            free -h >> {log.out}
            echo "System load:" >> {log.out}
            uptime >> {log.out}
            echo "Top memory processes:" >> {log.out}
            ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -n 10 >> {log.out}
            
            # Forward error code
            exit $EXIT_CODE
        }}

        # Log job completion
        echo "Job completed successfully: $(date)" >> {log.out}
        echo "Available system memory after execution:" >> {log.out}
        free -h >> {log.out}
        
        # Log output file sizes
        echo "Output file sizes:" >> {log.out}
        if [ -f "{output.consensus}" ] && [ -f "{output.alignment}" ]; then
            du -h {output.consensus} {output.alignment} >> {log.out}
        else
            echo "Warning: Output files not found. Check for errors." >> {log.out}
        fi
        """

# MGE rule for concat mode
rule MitoGeneExtractor_concat:
    input:
        DNA=get_mge_input_concat,
        AA=lambda wildcards: sequence_references[wildcards.sample]["protein_path"]
    output:
        alignment=os.path.join(output_dir_concat, "alignment/{sample}_r_{r}_s_{s}_align_{reference_name}.fas"),
        consensus=os.path.join(output_dir_concat, "consensus/{sample}_r_{r}_s_{s}_con_{reference_name}.fas"),
        vulgar=os.path.join(output_dir_concat, "logs/mge/{sample}_vulgar_r_{r}_s_{s}_{reference_name}.txt")
    log:
        out=os.path.join(output_dir_concat, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
        err=os.path.join(output_dir_concat, "err/{sample}_r_{r}_s_{s}_summary_{reference_name}.err")
    params:
        mge_executor=config["mge_path"],
        output_dir=output_dir_concat
    threads: lambda wildcards: get_cluster_threads("MitoGeneExtractor_concat")
    resources:
        mem_mb=lambda wildcards, attempt: (get_mem_mb_from_config("MitoGeneExtractor_concat")) * attempt
    retries: 3
    shell:
        """
        # Clear previous log file for clean output on retries
        > {log.out}
        
        # Logging header with improved attempt tracking
        echo "===== Job Info =====" >> {log.out}
        echo "Started: $(date)" >> {log.out}
        echo "Node: $(hostname)" >> {log.out}
        echo "Memory allocated: {resources.mem_mb}MB" >> {log.out}
        echo "Threads: {threads}" >> {log.out}
        echo "Mode: concat" >> {log.out}
        echo "===================" >> {log.out}
        
        # Log input files sizes
        echo "Input DNA file size: $(du -h {input.DNA} | cut -f1)" >> {log.out}
        echo "Input AA file: {input.AA}" >> {log.out}
        echo "Available system memory before execution:" >> {log.out}
        free -h >> {log.out}
            
        # Run MGE & track resource usage
        /usr/bin/time -v {params.mge_executor} \\
        -q {input.DNA} -p {input.AA} \\
        -o {params.output_dir}/alignment/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_align_ \\
        -c {params.output_dir}/consensus/{wildcards.sample}_r_{wildcards.r}_s_{wildcards.s}_con_ \\
        -V {output.vulgar} \\
        -r {wildcards.r} -s {wildcards.s} \\
        -n 0 -C 5 -t 0.5 \\
        --verbosity 4 \\
        >> {log.out} 2>> {log.err} || {{
            # If the command fails, log error and system state
            EXIT_CODE=$?
            echo "MitoGeneExtractor failed with exit code $EXIT_CODE" >> {log.out}
            echo "Available system memory after failure:" >> {log.out}
            free -h >> {log.out}
            echo "System load:" >> {log.out}
            uptime >> {log.out}
            echo "Top memory processes:" >> {log.out}
            ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -n 10 >> {log.out}
        
		    # Forward error code
            exit $EXIT_CODE
        }}
        
        # Log job completion
        echo "Job completed successfully: $(date)" >> {log.out}
        echo "Available system memory after execution:" >> {log.out}
        free -h >> {log.out}
        
        # Log output file sizes
        echo "Output file sizes:" >> {log.out}
        if [ -f "{output.consensus}" ] && [ -f "{output.alignment}" ]; then
            du -h {output.consensus} {output.alignment} >> {log.out}
        else
            echo "Warning: Output files not found. Check for errors." >> {log.out}
        fi
        """
		



# Rename headers in consensus files for merge mode
rule rename_and_combine_cons_merge:
    input:
        consensus_files=all_mge_cons_merge
    output:
        complete=os.path.join(output_dir_merge, "logs/rename_complete.txt"),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta")
    log:
        os.path.join(output_dir_merge, "logs/rename_fasta.log")
    params:
        script=os.path.join(workflow.basedir, "scripts/rename_headers.py"),
        preprocessing_mode="merge"
    threads: lambda wildcards: get_cluster_threads("rename_and_combine_cons_merge")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("rename_and_combine_cons_merge")
    retries: 3
    shell:
        """ 
		
		# Check all input files exist
        for file in {input.consensus_files}; do
            if [ ! -f "$file" ]; then
                echo "Error: Required input file does not exist: $file" >> {log}
                echo "Cannot proceed with renaming and combining consensus files." >> {log}
				echo "If any MitoGeneExtractor jobs failed, please rerun the workflow to generate those files." >> {log}

                exit 1
            fi
        done
		
        # Run script
        python {params.script} \\
            --input-files {input.consensus_files} \\
            --concatenated-consensus {output.concat_cons} \\
            --complete-file={output.complete} \\
            --log-file={log} \\
            --preprocessing-mode={params.preprocessing_mode} \\
            --threads={threads}
    
        # Touch completion file to ensure timestamp is updated
        touch {output.complete}
        """

# Rename headers in consensus files for concat mode
rule rename_and_combine_cons_concat:
    input:
        consensus_files=all_mge_cons_concat
    output:
        complete=os.path.join(output_dir_concat, "logs/rename_complete.txt"),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta")
    log:
        os.path.join(output_dir_concat, "logs/rename_fasta.log")
    params:
        script=os.path.join(workflow.basedir, "scripts/rename_headers.py"),
        preprocessing_mode="concat"
    threads: lambda wildcards: get_cluster_threads("rename_and_combine_cons_concat")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("rename_and_combine_cons_concat")
    retries: 3
    shell:
        """ 
		
		# Check all input files exist
        for file in {input.consensus_files}; do
            if [ ! -f "$file" ]; then
                echo "Error: Required input file does not exist: $file" >> {log}
                echo "Cannot proceed with renaming and combining consensus files." >> {log}
				echo "If any MitoGeneExtractor jobs failed, please rerun the workflow to generate those files." >> {log}
                exit 1
            fi
        done
		
        # Run script
        python {params.script} \\
            --input-files {input.consensus_files} \\
            --concatenated-consensus {output.concat_cons} \\
            --complete-file={output.complete} \\
            --log-file={log} \\
            --preprocessing-mode={params.preprocessing_mode} \\
            --threads={threads}
    
        # Touch completion file to ensure timestamp is updated
        touch {output.complete}
        """





# Create list of alignment files for stats for merge mode
rule create_alignment_log_merge:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir_merge, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta")
    output:
        alignment_log=os.path.join(output_dir_merge, "logs/alignment_files.log")
    threads: lambda wildcards: get_cluster_threads("create_alignment_log_merge")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("create_alignment_log_merge")
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")

# Create list of alignment files for stats for concat mode
rule create_alignment_log_concat:
    input:
        alignment_files=lambda wildcards: glob.glob(os.path.join(output_dir_concat, "alignment/*_align_*.fas")),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta")
    output:
        alignment_log=os.path.join(output_dir_concat, "logs/alignment_files.log")
    threads: lambda wildcards: get_cluster_threads("create_alignment_log_concat")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("create_alignment_log_concat")
    run:
        alignment_files = input.alignment_files

        with open(output.alignment_log, 'w') as log_file:
            for file in alignment_files:
                log_file.write(f"{file}\n")







# Run fasta_cleaner script on MGE alignment files for merge mode
rule fasta_cleaner_merge:
    input:
        alignment_log=os.path.join(output_dir_merge, "logs/alignment_files.log"),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta")
    output:
        completion_flag=os.path.join(output_dir_merge, "logs/cleaning_complete.txt"),
        cleaning_csv=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        concat_consensus=os.path.join(output_dir_merge, "fasta_cleaner/all_consensus_sequences.fasta")
    params:
        output_dir=os.path.join(output_dir_merge, "fasta_cleaner"),
        script=os.path.join(workflow.basedir, "scripts/fasta_cleaner_mge.py"),
        consensus_threshold=fasta_cleaner_params["consensus_threshold"],
        human_threshold=fasta_cleaner_params["human_threshold"],
        at_difference=fasta_cleaner_params["at_difference"],
        reference_dir=fasta_cleaner_params.get("reference_dir", None),
        at_mode=fasta_cleaner_params["at_mode"],
        outlier_percentile=fasta_cleaner_params["outlier_percentile"],
        preprocessing_mode="merge",
        disable_flags=" ".join([
            "--disable_human" if fasta_cleaner_params["disable_human"] else "",
            "--disable_at" if fasta_cleaner_params["disable_at"] else "",
            "--disable_outliers" if fasta_cleaner_params["disable_outliers"] else ""
        ]).strip()
    log:
        os.path.join(output_dir_merge, "logs/fasta_cleaner.log")
    threads: lambda wildcards: get_cluster_threads("fasta_cleaner_merge")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("fasta_cleaner_merge")
    retries: 3
    shell:
        """
        # Debug log
        echo "Script path: {params.script}"
        
        # Create out dir
        mkdir -p {params.output_dir}
        
        # Create temp input dir & symlink to alignment files
        TMP_INPUT_DIR=$(mktemp -d)
        echo "Created temp dir: $TMP_INPUT_DIR"
        
        # Debug show content of alignment log
        echo "Content of alignment log:"
        cat {input.alignment_log}
        
        while IFS= read -r alignment_file; do
            echo "Processing: $alignment_file"
            if [ ! -f "$alignment_file" ]; then
                echo "Warning: File not found: $alignment_file" >&2
                continue
            fi
            ln -s "$(realpath "$alignment_file")" "$TMP_INPUT_DIR/$(basename "$alignment_file")"
        done < {input.alignment_log}
        
        # Debug show content of temp dir
        echo "Content of temp directory:"
        ls -la "$TMP_INPUT_DIR"

        # Construct reference dir arg
        REFERENCE_DIR_ARG=""
        if [ "{params.reference_dir}" != "None" ] && [ -n "{params.reference_dir}" ]; then
             REFERENCE_DIR_ARG="--reference_dir {params.reference_dir}"
        fi
        
        # Run script with set -x for debugging
        set -x
        python {params.script} \\
            -i "$TMP_INPUT_DIR" \\
            -o {params.output_dir} \\
            --consensus_threshold {params.consensus_threshold} \\
            --human_threshold {params.human_threshold} \\
            --at_difference {params.at_difference} \\
            --at_mode {params.at_mode} \\
            --percentile_threshold {params.outlier_percentile} \\
            --preprocessing {params.preprocessing_mode} \\
            {params.disable_flags} \\
            $REFERENCE_DIR_ARG \\
            2>&1 | tee {log}
        set +x
            
        # Cleanup temp dir
        rm -rf "$TMP_INPUT_DIR"
        
        # Touch completion file
        touch {output.completion_flag}
        """

# Run fasta_cleaner script on MGE alignment files for concat mode
rule fasta_cleaner_concat:
    input:
        alignment_log=os.path.join(output_dir_concat, "logs/alignment_files.log"),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta")
    output:
        completion_flag=os.path.join(output_dir_concat, "logs/cleaning_complete.txt"),
        cleaning_csv=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        concat_consensus=os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta")
    params:
        output_dir=os.path.join(output_dir_concat, "fasta_cleaner"),
        script=os.path.join(workflow.basedir, "scripts/fasta_cleaner_mge.py"),
        consensus_threshold=fasta_cleaner_params["consensus_threshold"],
        human_threshold=fasta_cleaner_params["human_threshold"],
        at_difference=fasta_cleaner_params["at_difference"],
        reference_dir=fasta_cleaner_params.get("reference_dir", None),
        at_mode=fasta_cleaner_params["at_mode"],
        outlier_percentile=fasta_cleaner_params["outlier_percentile"],
        preprocessing_mode="concat",
        disable_flags=" ".join([
            "--disable_human" if fasta_cleaner_params["disable_human"] else "",
            "--disable_at" if fasta_cleaner_params["disable_at"] else "",
            "--disable_outliers" if fasta_cleaner_params["disable_outliers"] else ""
        ]).strip()
    log:
        os.path.join(output_dir_concat, "logs/fasta_cleaner.log")
    threads: lambda wildcards: get_cluster_threads("fasta_cleaner_concat")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("fasta_cleaner_concat")
    retries: 3
    shell:
        """
        # Debug log
        echo "Script path: {params.script}"
        
        # Create out dir
        mkdir -p {params.output_dir}
        
        # Create temp input dir & symlink to alignment files
        TMP_INPUT_DIR=$(mktemp -d)
        echo "Created temp dir: $TMP_INPUT_DIR"
        
        # Debug show content of alignment log
        echo "Content of alignment log:"
        cat {input.alignment_log}
        
        while IFS= read -r alignment_file; do
            echo "Processing: $alignment_file"
            if [ ! -f "$alignment_file" ]; then
                echo "Warning: File not found: $alignment_file" >&2
                continue
            fi
            ln -s "$(realpath "$alignment_file")" "$TMP_INPUT_DIR/$(basename "$alignment_file")"
        done < {input.alignment_log}
        
        # Debug show content of temp dir
        echo "Content of temp directory:"
        ls -la "$TMP_INPUT_DIR"

        # Construct reference dir arg
        REFERENCE_DIR_ARG=""
        if [ "{params.reference_dir}" != "None" ] && [ -n "{params.reference_dir}" ]; then
             REFERENCE_DIR_ARG="--reference_dir {params.reference_dir}"
        fi
        
        # Run script with set -x for debugging
        set -x
        python {params.script} \\
            -i "$TMP_INPUT_DIR" \\
            -o {params.output_dir} \\
            --consensus_threshold {params.consensus_threshold} \\
            --human_threshold {params.human_threshold} \\
            --at_difference {params.at_difference} \\
            --at_mode {params.at_mode} \\
            --percentile_threshold {params.outlier_percentile} \\
			--preprocessing {params.preprocessing_mode} \\
            {params.disable_flags} \\
            $REFERENCE_DIR_ARG \\
            2>&1 | tee {log}
        set +x
            
        # Cleanup tem dir
        rm -rf "$TMP_INPUT_DIR"
        
        # Touch completion file
        touch {output.completion_flag}
        """








# Run Fasta_compare script on MGE and fasta_cleaner concatenated consensus multi-FASTAs
rule fasta_compare:
    input:
        fasta_concat=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        fasta_merge=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        clean_fasta_concat=os.path.join(output_dir_concat, "fasta_cleaner/all_consensus_sequences.fasta"),
        clean_fasta_merge=os.path.join(output_dir_merge, "fasta_cleaner/all_consensus_sequences.fasta"),
        stats_concat=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        stats_merge=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv")
    output:
        csv=os.path.join(main_output_dir, f"fasta_compare/{run_name}_fasta_compare.csv"),
        full_fasta=os.path.join(main_output_dir, f"fasta_compare/{run_name}_full_sequences.fasta"),
        barcode_fasta=os.path.join(main_output_dir, f"fasta_compare/{run_name}_barcode_sequences.fasta")
    params:
        script=os.path.join(workflow.basedir, "scripts/fasta_compare.py"),
        target_marker=fasta_compare_params["target"],
        verbosity=" ".join([
            "--verbose" if fasta_compare_params["verbose"] else ""
        ]).strip()
    log:
        os.path.join(main_output_dir, f"fasta_compare_{run_name}.log")
    threads: lambda wildcards: get_cluster_threads("fasta_compare")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("fasta_compare")
    retries: 3
    shell:
        """
        python {params.script} \
            --output-csv {output.csv} \
            --output-fasta {output.full_fasta} \
            --output-barcode {output.barcode_fasta} \
            --input {input.fasta_concat} {input.fasta_merge} {input.clean_fasta_concat} {input.clean_fasta_merge} \
            --target {params.target_marker} \
            --log-file {log} \
            {params.verbosity}
        """
		
		
		
		
		
		

# Extract, aggregate and calculate stats from inputs for merge mode
rule extract_stats_to_csv_merge:
    input:
        alignment_log=os.path.join(output_dir_merge, "logs/alignment_files.log"),
        out_files=expand(os.path.join(output_dir_merge, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
                         zip, 
                         sample=[s for s, _ in sample_reference_pairs],
                         reference_name=[ref for _, ref in sample_reference_pairs],
                         r=config["r"],
                         s=config["s"]),
        concat_cons=os.path.join(output_dir_merge, f"consensus/{run_name}_merge.fasta"),
        cleaning_csv=os.path.join(output_dir_merge, "fasta_cleaner/combined_statistics.csv"),
        cleaner_complete=os.path.join(output_dir_merge, "logs/cleaning_complete.txt")
    output:
        stats=os.path.join(output_dir_merge, f"{run_name}_merge.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/mge_stats.py"),
        out_dir=os.path.join(output_dir_merge, "out/"),
        log_dir=os.path.join(output_dir_merge, "logs")
    threads: lambda wildcards: get_cluster_threads("extract_stats_to_csv_merge")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("extract_stats_to_csv_merge")
    retries: 3
    shell:
        """
	# Run script
        python {params.script} -a {input.alignment_log} -o {output.stats} -od {params.out_dir} -c {input.cleaning_csv}

        # Relocate logs
        mv mge_stats.log {params.log_dir}/
        """

# Extract, aggregate and calculate stats from inputs for concat mode
rule extract_stats_to_csv_concat:
    input:
        alignment_log=os.path.join(output_dir_concat, "logs/alignment_files.log"),
        out_files=expand(os.path.join(output_dir_concat, "out/{sample}_r_{r}_s_{s}_summary_{reference_name}.out"),
                         zip, 
                         sample=[s for s, _ in sample_reference_pairs],
                         reference_name=[ref for _, ref in sample_reference_pairs],
                         r=config["r"],
                         s=config["s"]),
        concat_cons=os.path.join(output_dir_concat, f"consensus/{run_name}_concat.fasta"),
        cleaning_csv=os.path.join(output_dir_concat, "fasta_cleaner/combined_statistics.csv"),
        cleaner_complete=os.path.join(output_dir_concat, "logs/cleaning_complete.txt")
    output:
        stats=os.path.join(output_dir_concat, f"{run_name}_concat.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/mge_stats.py"),
        out_dir=os.path.join(output_dir_concat, "out/"),
        log_dir=os.path.join(output_dir_concat, "logs")
    threads: lambda wildcards: get_cluster_threads("extract_stats_to_csv_concat")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("extract_stats_to_csv_concat")
    retries: 3
    shell:
        """
		# Run script
        python {params.script} -a {input.alignment_log} -o {output.stats} -od {params.out_dir} -c {input.cleaning_csv}

        # Relocate logs
        mv mge_stats.log {params.log_dir}/
        """
		
		




# Concatenate merge and concat stats CSV files with column alignment
rule combine_stats_files:
    input:
        merge_stats=os.path.join(output_dir_merge, f"{run_name}_merge.csv"),
        concat_stats=os.path.join(output_dir_concat, f"{run_name}_concat.csv")
    output:
        combined_stats=os.path.join(main_output_dir, f"{run_name}_combined_stats.csv")
    params:
        script=os.path.join(workflow.basedir, "scripts/csv_combiner_mge.py")
    threads: lambda wildcards: get_cluster_threads("combine_stats_files")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("combine_stats_files")
    retries: 3
    shell:
        """
        python {params.script} \
            --input {input.merge_stats} {input.concat_stats} \
            --output {output.combined_stats}
        """	
		
		
		
#Barocding_summariser rule here?

		

# Clean up superfluous files
rule cleanup_files:
    input:
        # Merge mode inputs
        merge_summary_csv=os.path.join(output_dir_merge, f"{run_name}_merge.csv"),
        clean_headers_log=os.path.join(output_dir_merge, "logs/clean_headers.log"),
        # Concat mode inputs
        concat_summary_csv=os.path.join(output_dir_concat, f"{run_name}_concat.csv"),
        concat_logs=os.path.join(output_dir_concat, "logs/concat_reads.log"),
        trim_galore_logs=os.path.join(output_dir_concat, "logs/trim_galore.log")
    output:
        merge_complete=touch(os.path.join(output_dir_merge, "cleanup_complete.txt")),
        concat_complete=touch(os.path.join(output_dir_concat, "cleanup_complete.txt"))
    threads: lambda wildcards: get_cluster_threads("cleanup_files")
    resources:
        mem=lambda wildcards: get_mem_mb_from_config("cleanup_files")
    retries: 2
    run:
        # Function to clean up a specific mode's files
        def cleanup_mode_files(mode_dir, mode_name):
            removed_files = 0
            removed_dirs = []
            cleaned_dirs = []
            
            # Remove (empty) .log files in mge subdir - both modes
            mge_log_dir = os.path.join(mode_dir, "logs/mge")
            if os.path.exists(mge_log_dir):
                mge_log_files = glob.glob(os.path.join(mge_log_dir, "*.log"))
                for mge_log_file in mge_log_files:
                    print(f"[{mode_name}] Removing MGE log file: {mge_log_file}")
                    os.remove(mge_log_file)
                    removed_files += 1
                print(f"[{mode_name}] Removed {len(mge_log_files)} MGE log files")
                cleaned_dirs.append(mge_log_dir)
            else:
                print(f"[{mode_name}] MGE log directory not found: {mge_log_dir}")
            
            # Mode-specific cleanup
            if mode_name == "merge":
                # Clean up individual clean_headers log files
                clean_headers_dir = os.path.join(mode_dir, "logs/clean_headers")
                if os.path.exists(clean_headers_dir):
                    clean_headers_logs = glob.glob(os.path.join(clean_headers_dir, "*.log"))
                    for log_file in clean_headers_logs:
                        print(f"[{mode_name}] Removing clean_headers log file: {log_file}")
                        os.remove(log_file)
                        removed_files += 1
                    print(f"[{mode_name}] Removed {len(clean_headers_logs)} clean_headers log files")
                    cleaned_dirs.append(clean_headers_dir)
                    
                    # Remove the clean_headers subdirectory if it's empty
                    try:
                        if len(os.listdir(clean_headers_dir)) == 0:
                            os.rmdir(clean_headers_dir)
                            print(f"[{mode_name}] Removed empty directory: {clean_headers_dir}")
                            removed_dirs.append(clean_headers_dir)
                    except OSError as e:
                        print(f"[{mode_name}] Could not remove directory {clean_headers_dir}: {e}")
                else:
                    print(f"[{mode_name}] clean_headers directory not found: {clean_headers_dir}")
            
            elif mode_name == "concat":
                # Clean up concat logs
                concat_dir = os.path.join(mode_dir, "logs/concat")
                if os.path.exists(concat_dir):
                    concat_logs = glob.glob(os.path.join(concat_dir, "*.log"))
                    for log_file in concat_logs:
                        print(f"[{mode_name}] Removing concat log file: {log_file}")
                        os.remove(log_file)
                        removed_files += 1
                    print(f"[{mode_name}] Removed {len(concat_logs)} concat log files")
                    cleaned_dirs.append(concat_dir)
                    
                    # Remove concat subdir
                    try:
                        if len(os.listdir(concat_dir)) == 0:
                            os.rmdir(concat_dir)
                            print(f"[{mode_name}] Removed empty directory: {concat_dir}")
                            removed_dirs.append(concat_dir)
                    except OSError as e:
                        print(f"[{mode_name}] Could not remove directory {concat_dir}: {e}")
                else:
                    print(f"[{mode_name}] concat directory not found: {concat_dir}")
                
                # Clean up individual trim_galore log files
                trim_galore_dir = os.path.join(mode_dir, "logs/trim_galore")
                if os.path.exists(trim_galore_dir):
                    trim_galore_logs = glob.glob(os.path.join(trim_galore_dir, "*.log"))
                    for log_file in trim_galore_logs:
                        print(f"[{mode_name}] Removing trim_galore log file: {log_file}")
                        os.remove(log_file)
                        removed_files += 1
                    print(f"[{mode_name}] Removed {len(trim_galore_logs)} trim_galore log files")
                    cleaned_dirs.append(trim_galore_dir)
                    
                    # Remove trim_galore subdir
                    try:
                        if len(os.listdir(trim_galore_dir)) == 0:
                            os.rmdir(trim_galore_dir)
                            print(f"[{mode_name}] Removed empty directory: {trim_galore_dir}")
                            removed_dirs.append(trim_galore_dir)
                    except OSError as e:
                        print(f"[{mode_name}] Could not remove directory {trim_galore_dir}: {e}")
                else:
                    print(f"[{mode_name}] trim_galore directory not found: {trim_galore_dir}")
            
            # Return cleanup statistics
            return {
                "removed_files": removed_files,
                "removed_dirs": removed_dirs,
                "cleaned_dirs": cleaned_dirs
            }
        
        # Cleanup for merge mode
        merge_stats = cleanup_mode_files(output_dir_merge, "merge")
        
        # Write completion message and cleanup info to merge output file
        with open(output.merge_complete, 'w') as f:
            f.write("Cleanup complete at " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            f.write(f"Total log files removed: {merge_stats['removed_files']}\n\n")
            
            f.write("Directories fully removed:\n")
            if merge_stats['removed_dirs']:
                for dir_path in merge_stats['removed_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were fully removed\n")
            
            f.write("\nDirectories cleaned of log files:\n")
            if merge_stats['cleaned_dirs']:
                for dir_path in merge_stats['cleaned_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were cleaned\n")
            
            f.write("\nPreprocessing mode: merge")
        
        # Cleanup for concat mode
        concat_stats = cleanup_mode_files(output_dir_concat, "concat")
        
        # Write completion message and cleanup info to concat output file
        with open(output.concat_complete, 'w') as f:
            f.write("Cleanup complete at " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
            
            f.write(f"Total log files removed: {concat_stats['removed_files']}\n\n")
            
            f.write("Directories fully removed:\n")
            if concat_stats['removed_dirs']:
                for dir_path in concat_stats['removed_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were fully removed\n")
            
            f.write("\nDirectories cleaned of log files:\n")
            if concat_stats['cleaned_dirs']:
                for dir_path in concat_stats['cleaned_dirs']:
                    f.write(f"- {dir_path}\n")
            else:
                f.write("- No directories were cleaned\n")
            
            f.write("\nPreprocessing mode: concat")
